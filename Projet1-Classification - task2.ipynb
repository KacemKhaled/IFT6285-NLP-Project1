{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h7FDXiieRla9"
   },
   "source": [
    "# Projet 1 - Classification \n",
    "\n",
    "* **Authors:** Mouna Dhaouadi / Kacem Khaled\n",
    "* **Class:** IFT6285\n",
    "* **Term:** Fall 2021\n",
    "* **Professor** Dr. Philippe Langlais"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBL4CY53n8iQ"
   },
   "source": [
    "### Connecting to drive "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25721,
     "status": "ok",
     "timestamp": 1635638738724,
     "user": {
      "displayName": "Kacem KHALED",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitiT29UmZr-UQllA9xaLkeKg08QGTyQ-u2AO4Zog=s64",
      "userId": "06931647234887804606"
     },
     "user_tz": 240
    },
    "id": "xDvgA0oKTJBH",
    "outputId": "929229a0-73a6-418c-ccc7-f4282eec55c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 336,
     "status": "ok",
     "timestamp": 1635639077144,
     "user": {
      "displayName": "Kacem KHALED",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitiT29UmZr-UQllA9xaLkeKg08QGTyQ-u2AO4Zog=s64",
      "userId": "06931647234887804606"
     },
     "user_tz": 240
    },
    "id": "LrRGlZRwlyO6",
    "outputId": "bf08d6f8-a9f6-4ab8-bbfb-0c40e5b71ae3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Copy of text-transformers.ipynb'   Models\n",
      " Data\t\t\t\t    Outputs\n",
      " dataframes.ipynb\t\t   'Projet1-Classification - task1.ipynb'\n",
      "'devoir1 (1).ipynb'\t\t   'Projet1-Classification - task2.ipynb'\n",
      " devoir1.ipynb\t\t\t    TP2.ipynb\n",
      " Links.gdoc\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "kacemdir = \"/content/drive/My Drive/1-Polymtl/IFT6285\"\n",
    "mounadir=  \"/content/drive/My Drive/IFT6285\"\n",
    "os.chdir(kacemdir)\n",
    "!ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JP7aIIPLSt0a"
   },
   "source": [
    "## Task 1 : COLA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oacawg-QTTaC"
   },
   "source": [
    "### Step1 : Download and get to know the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 137,
     "status": "ok",
     "timestamp": 1635639021523,
     "user": {
      "displayName": "Kacem KHALED",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GitiT29UmZr-UQllA9xaLkeKg08QGTyQ-u2AO4Zog=s64",
      "userId": "06931647234887804606"
     },
     "user_tz": 240
    },
    "id": "Y3HgHIr5vuTs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "PNxCl0D1oGD4"
   },
   "outputs": [],
   "source": [
    "COLA_data_folder = 'Data/CoLA/'\n",
    "COLA_models_folder  = 'Models/CoLA/'\n",
    "COLA_output_folder = 'Outputs/CoLA/'\n",
    "\n",
    "train_cola_file = COLA_data_folder +'train.tsv'\n",
    "dev_cola_file =COLA_data_folder + 'dev.tsv'\n",
    "test_cola_file =COLA_data_folder  + 'test.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1635632493317,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "a68OaM-HonlJ",
    "outputId": "5bd71cfd-b645-44d6-aaeb-daf43f9a3b74"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acceptability judgment label</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>0</td>\n",
       "      <td>Poseidon appears to own a dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>0</td>\n",
       "      <td>Digitize is my happiest memory</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>1</td>\n",
       "      <td>It is easy to slay the Gorgon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>1</td>\n",
       "      <td>I had the strangest feeling that I knew you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>1</td>\n",
       "      <td>What all did you get for Christmas?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8551 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Acceptability judgment label  \\\n",
       "0                                1   \n",
       "1                                1   \n",
       "2                                1   \n",
       "3                                1   \n",
       "4                                1   \n",
       "...                            ...   \n",
       "8546                             0   \n",
       "8547                             0   \n",
       "8548                             1   \n",
       "8549                             1   \n",
       "8550                             1   \n",
       "\n",
       "                                               Sentence  \n",
       "0     Our friends won't buy this analysis, let alone...  \n",
       "1     One more pseudo generalization and I'm giving up.  \n",
       "2      One more pseudo generalization or I'm giving up.  \n",
       "3        The more we study verbs, the crazier they get.  \n",
       "4             Day by day the facts are getting murkier.  \n",
       "...                                                 ...  \n",
       "8546                   Poseidon appears to own a dragon  \n",
       "8547                     Digitize is my happiest memory  \n",
       "8548                     It is easy to slay the Gorgon.  \n",
       "8549       I had the strangest feeling that I knew you.  \n",
       "8550                What all did you get for Christmas?  \n",
       "\n",
       "[8551 rows x 2 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cola_data =  pd.read_table(train_cola_file, header=None, usecols=[1,3] , names = [ 'Acceptability judgment label' , 'Sentence'])\n",
    "train_cola_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1635632245808,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "g98IRZQUooII",
    "outputId": "8d9a6fea-28d5-4782-e6c1-701f0ca47225"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6023\n",
       "0    2528\n",
       "Name: Acceptability judgment label, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cola_data['Acceptability judgment label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 989,
     "status": "ok",
     "timestamp": 1635632249313,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "zWn_424jA7dU",
    "outputId": "497aafc0-c2e2-4db6-8128-4ac352ffe41c"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'seaborn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\MobaXterm\\slash\\var\\log\\xwin/ipykernel_11584/93395047.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcountplot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mcountplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Acceptability judgment label'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cola_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'seaborn'"
     ]
    }
   ],
   "source": [
    "from seaborn import countplot\n",
    "countplot(x='Acceptability judgment label', data = train_cola_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1635561009523,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "ElAIduZEooWg",
    "outputId": "52abf513-352b-47bd-d53d-df6c90e39de0"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acceptability judgment label</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>They drank the pub.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>The professor talked us.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>We yelled ourselves.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>We yelled Harry hoarse.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>Harry coughed himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8531</th>\n",
       "      <td>0</td>\n",
       "      <td>Anson believed to be happy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8539</th>\n",
       "      <td>0</td>\n",
       "      <td>Anson left before Jenny saw himself.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8545</th>\n",
       "      <td>0</td>\n",
       "      <td>Anson thought that himself was going to the club.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8546</th>\n",
       "      <td>0</td>\n",
       "      <td>Poseidon appears to own a dragon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8547</th>\n",
       "      <td>0</td>\n",
       "      <td>Digitize is my happiest memory</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2528 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Acceptability judgment label                                           Sentence\n",
       "18                               0                                They drank the pub.\n",
       "20                               0                           The professor talked us.\n",
       "22                               0                               We yelled ourselves.\n",
       "23                               0                            We yelled Harry hoarse.\n",
       "25                               0                             Harry coughed himself.\n",
       "...                            ...                                                ...\n",
       "8531                             0                        Anson believed to be happy.\n",
       "8539                             0               Anson left before Jenny saw himself.\n",
       "8545                             0  Anson thought that himself was going to the club.\n",
       "8546                             0                   Poseidon appears to own a dragon\n",
       "8547                             0                     Digitize is my happiest memory\n",
       "\n",
       "[2528 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cola_data_wrong = train_cola_data.loc[train_cola_data['Acceptability judgment label'].isin([0])]\n",
    "train_cola_data_wrong"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 191,
     "status": "ok",
     "timestamp": 1635561012079,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "iWIa3nrxq12X",
    "outputId": "1d98a585-e0e0-44bf-b4f2-1fafb35a154f"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acceptability judgment label</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Our friends won't buy this analysis, let alone...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization and I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>One more pseudo generalization or I'm giving up.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>The more we study verbs, the crazier they get.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>Day by day the facts are getting murkier.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8543</th>\n",
       "      <td>1</td>\n",
       "      <td>Gilgamesh perhaps should be leaving.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8544</th>\n",
       "      <td>1</td>\n",
       "      <td>Gilgamesh hasn't kissed Ishtar.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8548</th>\n",
       "      <td>1</td>\n",
       "      <td>It is easy to slay the Gorgon.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8549</th>\n",
       "      <td>1</td>\n",
       "      <td>I had the strangest feeling that I knew you.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8550</th>\n",
       "      <td>1</td>\n",
       "      <td>What all did you get for Christmas?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6023 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Acceptability judgment label                                           Sentence\n",
       "0                                1  Our friends won't buy this analysis, let alone...\n",
       "1                                1  One more pseudo generalization and I'm giving up.\n",
       "2                                1   One more pseudo generalization or I'm giving up.\n",
       "3                                1     The more we study verbs, the crazier they get.\n",
       "4                                1          Day by day the facts are getting murkier.\n",
       "...                            ...                                                ...\n",
       "8543                             1               Gilgamesh perhaps should be leaving.\n",
       "8544                             1                    Gilgamesh hasn't kissed Ishtar.\n",
       "8548                             1                     It is easy to slay the Gorgon.\n",
       "8549                             1       I had the strangest feeling that I knew you.\n",
       "8550                             1                What all did you get for Christmas?\n",
       "\n",
       "[6023 rows x 2 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cola_data_right = train_cola_data.loc[train_cola_data['Acceptability judgment label'].isin([1])]\n",
    "train_cola_data_right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1635561014636,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "g2CbVkTIr-o8",
    "outputId": "8b1d8724-9562-456e-d98b-84b7c850d27d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 6, 40.700736755934976)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_sentences_list =  [sent for sent in train_cola_data[\"Sentence\"]]\n",
    "cola_max_len = len ( max(cola_sentences_list, key=len) ) \n",
    "cola_min_len = len ( min(cola_sentences_list, key=len) )\n",
    "cola_avg_len = mean ( [ len(sent) for sent in cola_sentences_list ]  )\n",
    "cola_max_len, cola_min_len, cola_avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1635561018342,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "rF7zZ9u7v9Z7",
    "outputId": "56753ce4-54a5-4cc9-baff-ef2334342ecd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(151, 6, 40.71558544303797)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_wrong_sentences_list =  [sent for sent in train_cola_data_wrong[\"Sentence\"]]\n",
    "cola_wrong_max_len = len ( max(cola_wrong_sentences_list, key=len) ) \n",
    "cola_wrong_min_len = len ( min(cola_wrong_sentences_list, key=len) )\n",
    "cola_wrong_avg_len = mean ( [ len(sent) for sent in cola_wrong_sentences_list ]  )\n",
    "cola_wrong_max_len, cola_wrong_min_len, cola_wrong_avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1635561018773,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "GAPBXQ7Jv9GY",
    "outputId": "654a0643-bb48-4d66-b459-401afe2d63bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(231, 6, 40.69450439980076)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cola_right_sentences_list =  [sent for sent in train_cola_data_right[\"Sentence\"]]\n",
    "cola_right_max_len = len ( max(cola_right_sentences_list, key=len) ) \n",
    "cola_right_min_len = len ( min(cola_right_sentences_list, key=len) )\n",
    "cola_right_avg_len = mean ( [ len(sent) for sent in cola_right_sentences_list ]  )\n",
    "cola_right_max_len, cola_right_min_len, cola_right_avg_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "executionInfo": {
     "elapsed": 384,
     "status": "ok",
     "timestamp": 1635561074817,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "rcjA8_arCHdx",
    "outputId": "c8011e4b-331f-4808-d1c1-ef74ef14a2db"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_for_boxplots = [ [ len(sent) for sent in cola_sentences_list ], [ len(sent) for sent in cola_wrong_sentences_list ], [ len(sent) for sent in cola_right_sentences_list ]]\n",
    "fig_boxplot, ax_boxplot = plt.subplots()\n",
    "ax_boxplot.set_title(' Sentences lengths distribution in the Cola train set')\n",
    "ax_boxplot.boxplot(data_for_boxplots, patch_artist = True)\n",
    "plt.xticks([1, 2, 3], [\"All sentences \", \"Grammatically wrong sentences\", \"Grammatically right sentences\"], rotation=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 316,
     "status": "ok",
     "timestamp": 1635634658902,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "h8KYIvqdv8-R",
    "outputId": "960e7b1d-6a0c-418a-9407-89cc654ad728"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Acceptability judgment label</th>\n",
       "      <th>Sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>The sailors rode the breeze clear of the rocks.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>The weights made the rope stretch over the pulley.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>The mechanical doll wriggled itself loose.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>If you had eaten more, you would want less.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>As you eat the most, you want the least.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1038</th>\n",
       "      <td>1</td>\n",
       "      <td>John considers Bill silly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1039</th>\n",
       "      <td>1</td>\n",
       "      <td>John considers Bill to be silly.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1040</th>\n",
       "      <td>0</td>\n",
       "      <td>John bought a dog for himself to play with.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1041</th>\n",
       "      <td>1</td>\n",
       "      <td>John arranged for himself to get the prize.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1042</th>\n",
       "      <td>1</td>\n",
       "      <td>John talked to Bill about himself.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1043 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Acceptability judgment label                                            Sentence\n",
       "0                                1     The sailors rode the breeze clear of the rocks.\n",
       "1                                1  The weights made the rope stretch over the pulley.\n",
       "2                                1          The mechanical doll wriggled itself loose.\n",
       "3                                1         If you had eaten more, you would want less.\n",
       "4                                0            As you eat the most, you want the least.\n",
       "...                            ...                                                 ...\n",
       "1038                             1                          John considers Bill silly.\n",
       "1039                             1                    John considers Bill to be silly.\n",
       "1040                             0         John bought a dog for himself to play with.\n",
       "1041                             1         John arranged for himself to get the prize.\n",
       "1042                             1                  John talked to Bill about himself.\n",
       "\n",
       "[1043 rows x 2 columns]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#todo: do the same for the dev set + put the statitisic in tables like slide 4 of sklearn PDF in the report  + put in the report \n",
    "dev_cola_data =  pd.read_table(dev_cola_file, header=None, usecols=[1,3] , names = [ 'Acceptability judgment label' , 'Sentence'])\n",
    "dev_cola_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kOmiwLVsT3-D"
   },
   "source": [
    "### Step 2 : Use a reasonable baseline and evaluate it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_srK8t9ww76p"
   },
   "outputs": [],
   "source": [
    "from sklearn.dummy import DummyClassifier\n",
    "from joblib import dump, load\n",
    "import pickle\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score, average_precision_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HLbZswvJ3jxz"
   },
   "source": [
    "#### Dummy Classifier to start with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 537,
     "status": "ok",
     "timestamp": 1635561122901,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "lGTBWIOizZIK",
    "outputId": "e37fbb8e-732f-4134-e1c5-a205fc6e40e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       322\n",
      "           1       0.69      1.00      0.82       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.35      0.50      0.41      1043\n",
      "weighted avg       0.48      0.69      0.57      1043\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# Dummy classifier  train \n",
    "dummy_classifier = DummyClassifier(strategy='most_frequent')\n",
    "dummy_classifier.fit(train_cola_data['Sentence'], train_cola_data['Acceptability judgment label'])\n",
    "\n",
    "#save model \n",
    "dump(dummy_classifier, COLA_models_folder+'dummy-most.clf')\n",
    "\n",
    "# load model \n",
    "clf_dummy = load(COLA_models_folder+'dummy-most.clf')\n",
    "\n",
    "# predict with the model\n",
    "y_dummy = clf_dummy.predict(dev_cola_data['Sentence']) # predictions\n",
    "\n",
    "# output and save  the prediction\n",
    "out = COLA_output_folder+\"dummy_model.out\"\n",
    "pickle.dump([clf_dummy.classes_, y_dummy], open(out, 'wb'))\n",
    "\n",
    "#open and read picle file to get predictions \n",
    "[ clf_dummy.classes_,y_dummy1 ] = pickle.load( open(out, 'rb'))\n",
    "\n",
    "#compare and evaluate the predictions\n",
    "print(classification_report(dev_cola_data['Acceptability judgment label'], y_dummy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0UF7tD15nvT"
   },
   "source": [
    "#### Reseanable baseline : Logistic Regression with TF-IDF vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "onbBEA_o5t_v"
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1351,
     "status": "ok",
     "timestamp": 1635561137039,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "jx3ltMZK5uNk",
    "outputId": "2ecbc5cf-18d4-42e5-ef61-850f5cd314cf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.06      0.10       322\n",
      "           1       0.70      0.97      0.81       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.56      0.51      0.46      1043\n",
      "weighted avg       0.61      0.69      0.59      1043\n",
      "\n",
      "accuracy score 0.6855225311601151\n",
      "average precision score 0.6965087102573256\n",
      "f1 score 0.8093023255813954\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_cola_data['Sentence']) \n",
    "\n",
    "LogisticRegression_classifier = LogisticRegression()\n",
    "LogisticRegression_classifier.fit(X_train, train_cola_data['Acceptability judgment label'])\n",
    "\n",
    "# dump the vectorizer and the model (for use at test time)\n",
    "dump(vectorizer, COLA_models_folder + 'tf-idf_vectorizer.vec')\n",
    "dump(LogisticRegression_classifier, COLA_models_folder + 'LogisticRegression_classifier.clf') \n",
    "\n",
    "# load model + vec\n",
    "vectorizer = load(COLA_models_folder + 'tf-idf_vectorizer.vec')\n",
    "LogisticRegression_classifier = load(COLA_models_folder+'LogisticRegression_classifier.clf')\n",
    "\n",
    "# predict with the model\n",
    "X_test = vectorizer.transform(dev_cola_data['Sentence']) # apply the vectorizer\n",
    "y_LR = LogisticRegression_classifier.predict(X_test) # run the classifier\n",
    "\n",
    "# output and save  the prediction\n",
    "out = COLA_output_folder+\"LogisticRegression_model.out\"\n",
    "pickle.dump([LogisticRegression_classifier.classes_, y_LR], open(out, 'wb'))\n",
    "\n",
    "#open and read pickle file to get predictions \n",
    "[ LogisticRegression_classifier.classes_,y_LR ] = pickle.load( open(out, 'rb'))\n",
    "\n",
    "#compare and evaluate the predictions\n",
    "print(classification_report(dev_cola_data['Acceptability judgment label'], y_LR))\n",
    "\n",
    "print('accuracy score', accuracy_score(dev_cola_data['Acceptability judgment label'], y_LR))\n",
    "print('average precision score', average_precision_score(dev_cola_data['Acceptability judgment label'], y_LR) )\n",
    "print('f1 score', f1_score(dev_cola_data['Acceptability judgment label'], y_LR))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1635561234727,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "X4jO34mHYVkZ",
    "outputId": "da455310-6fc5-4508-e9ed-049bcf4ab64d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855225311601151 0.8093023255813954\n"
     ]
    }
   ],
   "source": [
    "baseline_vectorizer = vectorizer\n",
    "baseline_classifier = LogisticRegression_classifier\n",
    "\n",
    "baseline_accuracy = accuracy_score(dev_cola_data['Acceptability judgment label'], y_LR)\n",
    "baseline_f1_score = f1_score(dev_cola_data['Acceptability judgment label'], y_LR)\n",
    "\n",
    "print(baseline_accuracy, baseline_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_9MBsg85ukI"
   },
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "different scoring metrics :  https://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1635561257624,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "lSdpP9ZwAGQI",
    "outputId": "869ae8db-31e2-44d1-a071-095a6f520cb5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<sklearn.metrics._plot.confusion_matrix.ConfusionMatrixDisplay at 0x7ff40715e450>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(dev_cola_data['Acceptability judgment label'], y_LR, labels=baseline_classifier.classes_)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n",
    "                               display_labels=baseline_classifier.classes_)\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09WZB71FUEKa"
   },
   "source": [
    "### Step3 : Trying to improve the baselines with various curious ways"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "34Ip8xXB92Wv"
   },
   "source": [
    " #### investigated the use of Cross Validation (training) --> it did not work , maybe remove this part later, I think it is irelevant. the performance on the test set did not change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12856,
     "status": "ok",
     "timestamp": 1635562154721,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "DQYa5xtEE6H-",
    "outputId": "2bde6c23-929b-4d64-8d09-3ddd1199e141"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.70327103 0.64719626 0.64953271 0.5911215  0.66588785 0.64719626\n",
      " 0.62383178 0.6588785  0.6682243  0.62616822 0.66588785 0.6440281\n",
      " 0.54332553 0.6440281  0.59953162 0.68149883 0.67915691 0.68384075\n",
      " 0.68852459 0.70257611]\n",
      "Accuracy: 0.651\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.06      0.10       322\n",
      "           1       0.70      0.97      0.81       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.56      0.51      0.46      1043\n",
      "weighted avg       0.61      0.69      0.59      1043\n",
      "\n",
      "accuracy score 0.6855225311601151\n",
      "average precision score 0.6965087102573256\n",
      "f1 score 0.8093023255813954\n",
      "##########\n",
      "[0.5766375  0.34367246 0.4084596  0.34333333 0.43110561 0.40719697\n",
      " 0.40684043 0.36901837 0.34624697 0.37907851 0.41547246 0.34289277\n",
      " 0.33148046 0.34289277 0.356387   0.42130751 0.38749535 0.48329463\n",
      " 0.51083591 0.51926101]\n",
      "Accuracy: 0.406\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.06      0.10       322\n",
      "           1       0.70      0.97      0.81       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.56      0.51      0.46      1043\n",
      "weighted avg       0.61      0.69      0.59      1043\n",
      "\n",
      "accuracy score 0.6855225311601151\n",
      "average precision score 0.6965087102573256\n",
      "f1 score 0.8093023255813954\n",
      "##########\n",
      "[0.50759487 0.45860927 0.4695154  0.42254166 0.48252544 0.4692364\n",
      " 0.45717686 0.47071442 0.47508306 0.45201036 0.48024956 0.45681063\n",
      " 0.38768918 0.45681063 0.42986342 0.48800295 0.4840347  0.49658546\n",
      " 0.50221484 0.50064599]\n",
      "Accuracy: 0.467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.06      0.10       322\n",
      "           1       0.70      0.97      0.81       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.56      0.51      0.46      1043\n",
      "weighted avg       0.61      0.69      0.59      1043\n",
      "\n",
      "accuracy score 0.6855225311601151\n",
      "average precision score 0.6965087102573256\n",
      "f1 score 0.8093023255813954\n",
      "##########\n",
      "[0.44155751 0.3929078  0.4178666  0.37678374 0.42507163 0.41683586\n",
      " 0.41681971 0.40364941 0.40056022 0.40213026 0.41900269 0.39173789\n",
      " 0.35667432 0.39173789 0.3855081  0.41904762 0.41139005 0.43927825\n",
      " 0.44758523 0.42016573]\n",
      "Accuracy: 0.409\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.06      0.10       322\n",
      "           1       0.70      0.97      0.81       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.56      0.51      0.46      1043\n",
      "weighted avg       0.61      0.69      0.59      1043\n",
      "\n",
      "accuracy score 0.6855225311601151\n",
      "average precision score 0.6965087102573256\n",
      "f1 score 0.8093023255813954\n",
      "##########\n",
      "Improvement of 0.000000 in accuracy compared to the baseline using scoring fuction = accuracy\n",
      "Improvement of 0.000000 in f1_score compared to the baseline using scoring fuction = accuracy\n",
      "Improvement of 0.000000 in accuracy compared to the baseline using scoring fuction = precision_macro\n",
      "Improvement of 0.000000 in f1_score compared to the baseline using scoring fuction = precision_macro\n",
      "Improvement of 0.000000 in accuracy compared to the baseline using scoring fuction = recall_macro\n",
      "Improvement of 0.000000 in f1_score compared to the baseline using scoring fuction = recall_macro\n",
      "Improvement of 0.000000 in accuracy compared to the baseline using scoring fuction = f1_macro\n",
      "Improvement of 0.000000 in f1_score compared to the baseline using scoring fuction = f1_macro\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "scoring = [ 'accuracy', 'precision_macro', 'recall_macro', 'f1_macro']\n",
    "accuracies = []\n",
    "f1_scores = []\n",
    "\n",
    "for score in scoring:\n",
    "  #cross validation on the train set\n",
    "  # #TODO change this with : cross_validate  \n",
    "  scores = cross_val_score(baseline_classifier, X_train, train_cola_data['Acceptability judgment label'], scoring=score, cv=20)\n",
    "  print (scores)\n",
    "  # report performance\n",
    "  print('Accuracy: %.3f' % mean(scores))\n",
    "\n",
    "  #on the test set now --> nothing changed\n",
    "  y_LR_CV = baseline_classifier.predict(X_test) # run the classifier\n",
    "  print(classification_report(dev_cola_data['Acceptability judgment label'], y_LR_CV))\n",
    "  print('accuracy score', accuracy_score(dev_cola_data['Acceptability judgment label'], y_LR_CV))\n",
    "  print('average precision score', average_precision_score(dev_cola_data['Acceptability judgment label'], y_LR_CV) )\n",
    "  print('f1 score', f1_score(dev_cola_data['Acceptability judgment label'], y_LR_CV))\n",
    "  print('##########')\n",
    "\n",
    "  #compare to the baseline\n",
    "  LR_CV_accuracy = accuracy_score(dev_cola_data['Acceptability judgment label'], y_LR_CV)\n",
    "  accuracies.append(LR_CV_accuracy)\n",
    "  LR_CV_f1_score = f1_score(dev_cola_data['Acceptability judgment label'], y_LR_CV)\n",
    "  f1_scores.append(LR_CV_f1_score)\n",
    "\n",
    "for index, score in enumerate(scoring):\n",
    "  # print(accuracies[index], baseline_accuracy, )\n",
    "  print(\"Improvement of %f in accuracy compared to the baseline using scoring fuction = %s\" % (  (100 * (accuracies[index] - baseline_accuracy) / baseline_accuracy) , score ) )\n",
    "  print('Improvement of %f in f1_score compared to the baseline using scoring fuction = %s' % ( (100 * (f1_scores[index] - baseline_f1_score) / baseline_f1_score), score ) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q-BDff1ZVGnu"
   },
   "source": [
    "#### Investigate the use of Grid search for LR classifier --> very minor improvement in accuracy and f1 score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22507,
     "status": "ok",
     "timestamp": 1635562497157,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "7OKmvVbwFI-u",
    "outputId": "8911eeb5-e3ad-4f45-9bb8-124c2ecc5591"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.5, 1, 1.5], 'class_weight': ['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n",
      "0.6869369295512925\n",
      "{'C': 0.5, 'class_weight': None, 'solver': 'newton-cg'}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.01      0.02       322\n",
      "           1       0.69      0.99      0.81       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.51      0.50      0.42      1043\n",
      "weighted avg       0.58      0.69      0.57      1043\n",
      "\n",
      "0.6874400767018217 0.8139269406392693 0.6855225311601151 0.8093023255813954\n",
      "Improvement of 0.28% in accuracy compared to the baseline.\n",
      "Improvement of 0.57% in f1_score compared to the baseline.\n"
     ]
    }
   ],
   "source": [
    "## using the gridSearch - for LR classifier , not the vectorizer, on the Train Set !!! \n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] # , default=’lbfgs’\n",
    "C_values =[0.5 ,1, 1.5]  # default=1.0\n",
    "class_weights = ['balanced', None] # default=None\n",
    "\n",
    "param_grid = dict(C=C_values, class_weight=class_weights , solver = solvers )\n",
    "print(param_grid)\n",
    "\n",
    "grid = GridSearchCV(baseline_classifier, param_grid, cv=10, scoring='accuracy')\n",
    "vectorizer = TfidfVectorizer()\n",
    "X_train = vectorizer.fit_transform(train_cola_data['Sentence']) \n",
    "grid.fit(X_train, train_cola_data['Acceptability judgment label'])\n",
    "\n",
    "#affichage \n",
    "pd.DataFrame(grid.cv_results_)[['mean_test_score', 'std_test_score', 'params']]\n",
    "\n",
    "# examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "# predict with the best parameteres \n",
    "X_test = vectorizer.transform(dev_cola_data['Sentence']) # apply the vectorizer\n",
    "y_LR_best = grid.predict(X_test) \n",
    "print(classification_report(dev_cola_data['Acceptability judgment label'], y_LR_best))\n",
    "\n",
    "\n",
    "  #compare to the baseline\n",
    "LR_best_accuracy = accuracy_score(dev_cola_data['Acceptability judgment label'], y_LR_best)\n",
    "LR_best_score = f1_score(dev_cola_data['Acceptability judgment label'],y_LR_best)\n",
    "\n",
    "print(LR_best_accuracy, LR_best_score, baseline_accuracy, baseline_f1_score)\n",
    "print('Improvement of {:0.2f}% in accuracy compared to the baseline.'.format( 100 * (LR_best_accuracy - baseline_accuracy) / baseline_accuracy))\n",
    "print('Improvement of {:0.2f}% in f1_score compared to the baseline.'.format( 100 * (LR_best_score - baseline_f1_score) / baseline_f1_score))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z00LeaC1ADRU"
   },
   "source": [
    "#### Impact of changing hyper-parmeters of the baseline - minor improvement in accuracy and f1 score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FoBVY7Rj_pKs"
   },
   "outputs": [],
   "source": [
    "def train_save_predict(vectorizer, classifer, vectorizer_name, classifier_name, vec_meta_parameters_name, clf_meta_parameters_name):\n",
    "    \n",
    "    X_train = vectorizer.fit_transform(train_cola_data['Sentence']) \n",
    "    classifer.fit(X_train, train_cola_data['Acceptability judgment label'])\n",
    "\n",
    "    # dump the vectorizer and the model (for use at test time)\n",
    "    dump(vectorizer, COLA_models_folder + vectorizer_name )\n",
    "    dump(classifier, COLA_models_folder + classifier_name) \n",
    "\n",
    "    # load model + vec\n",
    "    tf_idf_vectorizer = load(COLA_models_folder + vectorizer_name)\n",
    "    LR_classifier = load(COLA_models_folder+classifier_name)\n",
    "\n",
    "    # predict with the model\n",
    "    X_test = tf_idf_vectorizer.transform(dev_cola_data['Sentence']) # apply the vectorizer\n",
    "    y_LR = LR_classifier.predict(X_test) # run the classifier\n",
    "\n",
    "    # output and save  the prediction\n",
    "    out = COLA_output_folder+'LogisticRegression_classifier'+clf_meta_parameters_name+'_'+'tf-idf_vectorizer'+vec_meta_parameters_name+\".out\"\n",
    "    pickle.dump([LR_classifier.classes_, y_LR], open(out, 'wb'))\n",
    "\n",
    "\n",
    "\n",
    "###### Step 1 :  try different meta-parameters and save them in different files.\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "analyzers = ['word', 'char', 'char_wb'] # default is word\n",
    "ngram_ranges = [(1,1), (1, 2), (2,2)] #unigram or bigram  / bigram only /(default is unigram) (1,1)\n",
    "\n",
    "# https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer\n",
    "min_dfs = [0.01, 0.05 ,1.0 ] #  default=1 (does not ignore), ignore terms that appear in less than 1% of the documents\". % too infrequenct\n",
    "max_dfs = [1.0, 0.8 , 0.9 ] # default=1.0 (does not ignore), ignore terms that appear in more than 80% of the documents\" % too frequent\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] # , default=’lbfgs’\n",
    "C_values =[0.5 ,1, 1.5]  # default=1.0\n",
    "class_weights = ['balanced', None] # default=None\n",
    "\n",
    "\n",
    "for analyzer in analyzers:\n",
    "  for ngram_range in ngram_ranges:\n",
    "    for min_df in min_dfs:\n",
    "      for max_df in max_dfs:\n",
    "          print(max_df, min_df)\n",
    "          if max_df <= min_df: # we remove impossible combinations\n",
    "            pass\n",
    "          else:\n",
    "            for solver in solvers: \n",
    "              for c in C_values:\n",
    "                for class_weight in class_weights:\n",
    "                  # a number of options can control a vectorizer, I reckon you investigate them\n",
    "                  vectorizer = TfidfVectorizer(analyzer=analyzer, min_df=min_df,\n",
    "                                              max_df=max_df,\n",
    "                                              ngram_range=ngram_range)\n",
    "                  \n",
    "                  vec_meta_parameters_name = '_analyzer='+str(analyzer)+'_ngram_range='+str(ngram_range)+'_max_df='+str(max_df)+'_min_df='+str(min_df)\n",
    "                  vectorizer_name = 'tf-idf_vectorizer'+vec_meta_parameters_name+'.vec'\n",
    "\n",
    "                  classifier = LogisticRegression(C=c,class_weight=class_weight, solver=solver)\n",
    "                  clf_meta_parameters_name = '_C='+str(c)+'_class_weight='+ str(class_weight)+'_solver='+str(solver)\n",
    "\n",
    "                  classifier_name = 'LogisticRegression_classifier'+clf_meta_parameters_name+'_'+'tf-idf_vectorizer'+vec_meta_parameters_name+'.clf'\n",
    "                  print(classifier_name)\n",
    "                  try:\n",
    "                    train_save_predict(vectorizer, classifier, vectorizer_name, classifier_name, vec_meta_parameters_name, clf_meta_parameters_name )\n",
    "                  except ValueError as err:\n",
    "                    print(err)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 490
    },
    "executionInfo": {
     "elapsed": 5093,
     "status": "ok",
     "timestamp": 1635565028880,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "aSl9AFk0rBRR",
    "outputId": "3f165c48-1b9f-48df-b71e-2dff5b7f3918"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5263662511984659, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5254074784276127, 0.6903163950143816, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5302013422818792, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5282837967401726, 0.6903163950143816, 0.5321188878235859, 0.6893576222435283, 0.5292425695110259, 0.6903163950143816, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.6558005752636625, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5282837967401726, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5244487056567594, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5244487056567594, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5244487056567594, 0.6912751677852349, 0.5273250239693192, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5177372962607862, 0.6893576222435283, 0.5244487056567594, 0.6864813039309684, 0.5263662511984659, 0.6855225311601151, 0.5177372962607862, 0.6893576222435283, 0.5244487056567594, 0.6864813039309684, 0.5254074784276127, 0.6855225311601151, 0.5177372962607862, 0.6893576222435283, 0.5244487056567594, 0.6874400767018217, 0.5263662511984659, 0.6855225311601151, 0.5177372962607862, 0.6893576222435283, 0.5244487056567594, 0.6864813039309684, 0.5263662511984659, 0.6855225311601151, 0.5177372962607862, 0.6893576222435283, 0.5244487056567594, 0.6864813039309684, 0.5244487056567594, 0.6855225311601151, 0.5225311601150527, 0.6874400767018217, 0.5292425695110259, 0.6864813039309684, 0.5302013422818792, 0.6836049856184084, 0.5225311601150527, 0.6874400767018217, 0.5292425695110259, 0.6864813039309684, 0.5302013422818792, 0.6836049856184084, 0.5225311601150527, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5302013422818792, 0.6836049856184084, 0.5225311601150527, 0.6874400767018217, 0.5292425695110259, 0.6864813039309684, 0.5302013422818792, 0.6836049856184084, 0.5225311601150527, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5302013422818792, 0.6836049856184084, 0.5206136145733461, 0.6883988494726749, 0.5292425695110259, 0.6864813039309684, 0.5292425695110259, 0.6855225311601151, 0.5206136145733461, 0.6883988494726749, 0.5292425695110259, 0.6864813039309684, 0.5292425695110259, 0.6855225311601151, 0.5206136145733461, 0.6883988494726749, 0.5292425695110259, 0.6864813039309684, 0.5292425695110259, 0.6864813039309684, 0.5206136145733461, 0.6883988494726749, 0.5292425695110259, 0.6864813039309684, 0.5292425695110259, 0.6855225311601151, 0.5206136145733461, 0.6883988494726749, 0.5302013422818792, 0.6864813039309684, 0.5282837967401726, 0.6864813039309684, 0.5158197507190796, 0.6903163950143816, 0.5186960690316395, 0.6893576222435283, 0.5206136145733461, 0.6874400767018217, 0.5158197507190796, 0.6903163950143816, 0.5186960690316395, 0.6893576222435283, 0.5196548418024928, 0.6874400767018217, 0.5158197507190796, 0.6903163950143816, 0.5186960690316395, 0.6893576222435283, 0.5206136145733461, 0.6874400767018217, 0.5158197507190796, 0.6903163950143816, 0.5186960690316395, 0.6893576222435283, 0.5206136145733461, 0.6874400767018217, 0.5158197507190796, 0.6903163950143816, 0.5186960690316395, 0.6893576222435283, 0.5196548418024928, 0.6874400767018217, 0.5148609779482263, 0.6893576222435283, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6883988494726749, 0.5158197507190796, 0.6893576222435283, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6883988494726749, 0.5148609779482263, 0.6883988494726749, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6864813039309684, 0.5148609779482263, 0.6893576222435283, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6883988494726749, 0.5158197507190796, 0.6893576222435283, 0.5148609779482263, 0.6883988494726749, 0.5177372962607862, 0.6864813039309684, 0.5158197507190796, 0.6903163950143816, 0.5167785234899329, 0.6883988494726749, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6903163950143816, 0.5167785234899329, 0.6883988494726749, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6903163950143816, 0.5167785234899329, 0.6893576222435283, 0.513902205177373, 0.6883988494726749, 0.5158197507190796, 0.6903163950143816, 0.5167785234899329, 0.6883988494726749, 0.513902205177373, 0.6883988494726749, 0.5177372962607862, 0.6903163950143816, 0.5158197507190796, 0.6893576222435283, 0.513902205177373, 0.6883988494726749, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6855225311601151, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5186960690316395, 0.6883988494726749, 0.5158197507190796, 0.6855225311601151, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6855225311601151, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6855225311601151, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6855225311601151, 0.513902205177373, 0.6826462128475551, 0.5206136145733461, 0.6883988494726749, 0.5158197507190796, 0.6845637583892618, 0.513902205177373, 0.6826462128475551, 0.5186960690316395, 0.6883988494726749, 0.5158197507190796, 0.6855225311601151, 0.5148609779482263, 0.6826462128475551, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5167785234899329, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5167785234899329, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5100671140939598, 0.6903163950143816, 0.5129434324065196, 0.6903163950143816, 0.5177372962607862, 0.6874400767018217, 0.5110258868648131, 0.6903163950143816, 0.5119846596356663, 0.6903163950143816, 0.5167785234899329, 0.6874400767018217, 0.5254074784276127, 0.6912751677852349, 0.5186960690316395, 0.6912751677852349, 0.5196548418024928, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5186960690316395, 0.6912751677852349, 0.5196548418024928, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5177372962607862, 0.6912751677852349, 0.5196548418024928, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5186960690316395, 0.6912751677852349, 0.5196548418024928, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5177372962607862, 0.6912751677852349, 0.5215723873441994, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5349952061361457, 0.6912751677852349, 0.5340364333652924, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.535953978906999, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5321188878235859, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5282837967401726, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5292425695110259, 0.6912751677852349, 0.5302013422818792, 0.6912751677852349, 0.5311601150527325, 0.6912751677852349, 0.5225311601150527, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5225311601150527, 0.6912751677852349, 0.5244487056567594, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5225311601150527, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5244487056567594, 0.6912751677852349, 0.5244487056567594, 0.6912751677852349, 0.5234899328859061, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5263662511984659, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5254074784276127, 0.6912751677852349, 0.5225311601150527, 0.6903163950143816, 0.5292425695110259, 0.6883988494726749, 0.5397890699904123, 0.6874400767018217, 0.5215723873441994, 0.6903163950143816, 0.5292425695110259, 0.6883988494726749, 0.538830297219559, 0.6874400767018217, 0.5206136145733461, 0.6903163950143816, 0.5292425695110259, 0.6883988494726749, 0.5397890699904123, 0.6874400767018217, 0.5225311601150527, 0.6903163950143816, 0.5292425695110259, 0.6883988494726749, 0.5397890699904123, 0.6874400767018217, 0.5206136145733461, 0.6903163950143816, 0.5292425695110259, 0.6883988494726749, 0.538830297219559, 0.6874400767018217, 0.5292425695110259, 0.6883988494726749, 0.5378715244487057, 0.6855225311601151, 0.5378715244487057, 0.6836049856184084, 0.5292425695110259, 0.6883988494726749, 0.5378715244487057, 0.6855225311601151, 0.5378715244487057, 0.6836049856184084, 0.5292425695110259, 0.6883988494726749, 0.5378715244487057, 0.6845637583892618, 0.5378715244487057, 0.6836049856184084, 0.5292425695110259, 0.6883988494726749, 0.5378715244487057, 0.6855225311601151, 0.5378715244487057, 0.6836049856184084, 0.5292425695110259, 0.6883988494726749, 0.5369127516778524, 0.6845637583892618, 0.5378715244487057, 0.6836049856184084, 0.5302013422818792, 0.6893576222435283, 0.5378715244487057, 0.6855225311601151, 0.5369127516778524, 0.6836049856184084, 0.5302013422818792, 0.6893576222435283, 0.5378715244487057, 0.6855225311601151, 0.5369127516778524, 0.6836049856184084, 0.5302013422818792, 0.6893576222435283, 0.5378715244487057, 0.6855225311601151, 0.5369127516778524, 0.6836049856184084, 0.5302013422818792, 0.6893576222435283, 0.5378715244487057, 0.6855225311601151, 0.5369127516778524, 0.6836049856184084, 0.5302013422818792, 0.6893576222435283, 0.5378715244487057, 0.6855225311601151, 0.5369127516778524, 0.6836049856184084, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6893576222435283, 0.5330776605944392, 0.6874400767018217, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6893576222435283, 0.5330776605944392, 0.6874400767018217, 0.5273250239693192, 0.6912751677852349, 0.5282837967401726, 0.6893576222435283, 0.5330776605944392, 0.6874400767018217, 0.5263662511984659, 0.6912751677852349, 0.5263662511984659, 0.6893576222435283, 0.5330776605944392, 0.6874400767018217, 0.5273250239693192, 0.6912751677852349, 0.5292425695110259, 0.6893576222435283, 0.5330776605944392, 0.6874400767018217, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6864813039309684, 0.5282837967401726, 0.6855225311601151, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6864813039309684, 0.5282837967401726, 0.6855225311601151, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6864813039309684, 0.5282837967401726, 0.6855225311601151, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6864813039309684, 0.5282837967401726, 0.6855225311601151, 0.5263662511984659, 0.6903163950143816, 0.5282837967401726, 0.6864813039309684, 0.5302013422818792, 0.6855225311601151, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5263662511984659, 0.6903163950143816, 0.5282837967401726, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5263662511984659, 0.6903163950143816, 0.5292425695110259, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5254074784276127, 0.6903163950143816, 0.5282837967401726, 0.6874400767018217, 0.5292425695110259, 0.6874400767018217, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5292425695110259, 0.6836049856184084, 0.5177372962607862, 0.6874400767018217, 0.5282837967401726, 0.6826462128475551, 0.5292425695110259, 0.6836049856184084, 0.5177372962607862, 0.6874400767018217, 0.5282837967401726, 0.6826462128475551, 0.5292425695110259, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5292425695110259, 0.6826462128475551, 0.5292425695110259, 0.6836049856184084, 0.5177372962607862, 0.6874400767018217, 0.5282837967401726, 0.6826462128475551, 0.5292425695110259, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5292425695110259, 0.6826462128475551, 0.5302013422818792, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5186960690316395, 0.6874400767018217, 0.5273250239693192, 0.6826462128475551, 0.5282837967401726, 0.6836049856184084, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6883988494726749, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6883988494726749, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6874400767018217, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6883988494726749, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5129434324065196, 0.6874400767018217, 0.5091083413231065, 0.6903163950143816, 0.5110258868648131, 0.6903163950143816, 0.5100671140939598, 0.6874400767018217, 0.5091083413231065, 0.6903163950143816, 0.5100671140939598, 0.6903163950143816, 0.5100671140939598, 0.6874400767018217, 0.5091083413231065, 0.6903163950143816, 0.5100671140939598, 0.6903163950143816, 0.5100671140939598, 0.6874400767018217, 0.5091083413231065, 0.6903163950143816, 0.5110258868648131, 0.6903163950143816, 0.5100671140939598, 0.6874400767018217, 0.5091083413231065, 0.6903163950143816, 0.5100671140939598, 0.6903163950143816, 0.5119846596356663, 0.6874400767018217, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6883988494726749, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6883988494726749, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6874400767018217, 0.5119846596356663, 0.6903163950143816, 0.5158197507190796, 0.6903163950143816, 0.5110258868648131, 0.6883988494726749, 0.5119846596356663, 0.6903163950143816, 0.5148609779482263, 0.6903163950143816, 0.5119846596356663, 0.6874400767018217]\n",
      "[0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6164596273291926, 0.8165814877910278, 0.6153846153846154, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6159813809154383, 0.8165814877910278, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6195652173913044, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6174183514774494, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6168224299065421, 0.8165814877910278, 0.6211180124223602, 0.815909090909091, 0.6184926184926185, 0.816372939169983, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.6256643887623387, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.7796193984039289, 0.8174603174603174, 0.6022544283413849, 0.8174603174603174, 0.6072289156626507, 0.8174603174603174, 0.6078588612670408, 0.8174603174603174, 0.6022544283413849, 0.8174603174603174, 0.6072289156626507, 0.8174603174603174, 0.608974358974359, 0.8174603174603174, 0.6011281224818695, 0.8174603174603174, 0.6072289156626507, 0.8174603174603174, 0.6078588612670408, 0.8174603174603174, 0.6022544283413849, 0.8174603174603174, 0.6072289156626507, 0.8174603174603174, 0.6078588612670408, 0.8174603174603174, 0.6022544283413849, 0.8174603174603174, 0.6067415730337079, 0.8174603174603174, 0.608974358974359, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5953565505804312, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5980392156862745, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5968928863450531, 0.8174603174603174, 0.6014669926650367, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5999999999999999, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.6011281224818695, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.6027397260273973, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.6011281224818695, 0.8174603174603174, 0.5999999999999999, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.6011281224818695, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.6006441223832528, 0.8174603174603174, 0.6027397260273973, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5902720527617478, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5957446808510639, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5957446808510639, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5945945945945946, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5957446808510639, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5945945945945946, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.6023715415019762, 0.815489749430524, 0.610062893081761, 0.8130360205831905, 0.6116352201257862, 0.811277330264672, 0.6023715415019762, 0.815489749430524, 0.610062893081761, 0.8130360205831905, 0.6105428796223447, 0.811277330264672, 0.6023715415019762, 0.815489749430524, 0.610062893081761, 0.8135011441647597, 0.6116352201257862, 0.811277330264672, 0.6023715415019762, 0.815489749430524, 0.610062893081761, 0.8130360205831905, 0.6116352201257862, 0.811277330264672, 0.6023715415019762, 0.815489749430524, 0.610062893081761, 0.8130360205831905, 0.6094488188976378, 0.811277330264672, 0.6066350710900474, 0.8141391106043329, 0.6136900078678206, 0.8121769098219415, 0.6147798742138364, 0.8096885813148788, 0.6066350710900474, 0.8141391106043329, 0.6136900078678206, 0.8121769098219415, 0.6147798742138364, 0.8096885813148788, 0.6066350710900474, 0.8141391106043329, 0.6136900078678206, 0.8126436781609195, 0.6147798742138364, 0.8096885813148788, 0.6066350710900474, 0.8141391106043329, 0.6136900078678206, 0.8121769098219415, 0.6147798742138364, 0.8096885813148788, 0.6066350710900474, 0.8141391106043329, 0.6136900078678206, 0.8126436781609195, 0.6147798742138364, 0.8096885813148788, 0.6044303797468354, 0.814814814814815, 0.6142969363707778, 0.8126074498567335, 0.6136900078678206, 0.8110599078341014, 0.6044303797468354, 0.814814814814815, 0.6142969363707778, 0.8126074498567335, 0.6136900078678206, 0.8110599078341014, 0.6044303797468354, 0.814814814814815, 0.6142969363707778, 0.8126074498567335, 0.6136900078678206, 0.8115273775216137, 0.6044303797468354, 0.814814814814815, 0.6142969363707778, 0.8126074498567335, 0.6136900078678206, 0.8110599078341014, 0.6044303797468354, 0.814814814814815, 0.6147798742138364, 0.8126074498567335, 0.6125984251968504, 0.8115273775216137, 0.601420678768745, 0.8167895632444696, 0.6059654631083202, 0.815909090909091, 0.6069182389937107, 0.8143507972665147, 0.601420678768745, 0.8167895632444696, 0.6059654631083202, 0.815909090909091, 0.6058221872541305, 0.8143507972665147, 0.601420678768745, 0.8167895632444696, 0.6059654631083202, 0.815909090909091, 0.6069182389937107, 0.8143507972665147, 0.601420678768745, 0.8167895632444696, 0.6059654631083202, 0.815909090909091, 0.6069182389937107, 0.8143507972665147, 0.601420678768745, 0.8167895632444696, 0.6059654631083202, 0.815909090909091, 0.6058221872541305, 0.8143507972665147, 0.6009463722397476, 0.8161180476730988, 0.6004728132387707, 0.8152359295054008, 0.6045418950665623, 0.8152359295054008, 0.601420678768745, 0.8161180476730988, 0.6004728132387707, 0.8152359295054008, 0.6045418950665623, 0.8152359295054008, 0.6009463722397476, 0.8154457694491766, 0.6004728132387707, 0.8152359295054008, 0.6045418950665623, 0.8138873079112123, 0.6009463722397476, 0.8161180476730988, 0.6004728132387707, 0.8152359295054008, 0.6045418950665623, 0.8152359295054008, 0.601420678768745, 0.8161180476730988, 0.6015748031496063, 0.8152359295054008, 0.6061080657791699, 0.8138873079112123, 0.6032992930086409, 0.8167895632444696, 0.6031496062992125, 0.8152359295054008, 0.6011014948859166, 0.8150256118383608, 0.6032992930086409, 0.8167895632444696, 0.6031496062992125, 0.8152359295054008, 0.6011014948859166, 0.8150256118383608, 0.6032992930086409, 0.8167895632444696, 0.6031496062992125, 0.815909090909091, 0.6011014948859166, 0.8150256118383608, 0.6032992930086409, 0.8167895632444696, 0.6031496062992125, 0.8152359295054008, 0.6011014948859166, 0.8150256118383608, 0.6042486231313926, 0.8167895632444696, 0.6026750590086546, 0.815909090909091, 0.6011014948859166, 0.8150256118383608, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.811277330264672, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6028481012658228, 0.814814814814815, 0.6007905138339921, 0.811277330264672, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.811277330264672, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6050552922590837, 0.814814814814815, 0.6007905138339921, 0.811277330264672, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.811277330264672, 0.5998421468034727, 0.8085598611914401, 0.6044303797468354, 0.814814814814815, 0.6007905138339921, 0.8108108108108107, 0.5998421468034727, 0.8085598611914401, 0.6028481012658228, 0.814814814814815, 0.6007905138339921, 0.811277330264672, 0.6009463722397476, 0.8085598611914401, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6050156739811912, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6050156739811912, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.598586017282011, 0.8167895632444696, 0.6012558869701727, 0.816372939169983, 0.6061080657791699, 0.8143507972665147, 0.5996860282574569, 0.8167895632444696, 0.6007843137254901, 0.816372939169983, 0.6050156739811912, 0.8143507972665147, 0.5998383185125303, 0.8174603174603174, 0.5951612903225806, 0.8174603174603174, 0.5975903614457831, 0.8174603174603174, 0.5998383185125303, 0.8174603174603174, 0.5951612903225806, 0.8174603174603174, 0.5975903614457831, 0.8174603174603174, 0.6016129032258065, 0.8174603174603174, 0.5946817082997584, 0.8174603174603174, 0.5975903614457831, 0.8174603174603174, 0.5998383185125303, 0.8174603174603174, 0.5951612903225806, 0.8174603174603174, 0.5975903614457831, 0.8174603174603174, 0.6003236245954693, 0.8174603174603174, 0.5946817082997584, 0.8174603174603174, 0.5998396150761828, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.598842018196857, 0.8174603174603174, 0.5983471074380166, 0.8174603174603174, 0.5960264900662251, 0.8174603174603174, 0.5993377483443709, 0.8174603174603174, 0.5973597359735973, 0.8174603174603174, 0.5953565505804312, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5980392156862745, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.5991836734693877, 0.8174603174603174, 0.600326264274062, 0.8174603174603174, 0.6008163265306122, 0.8174603174603174, 0.5957792207792207, 0.8174603174603174, 0.5982215036378334, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.5957792207792207, 0.8174603174603174, 0.5993537964458805, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.5978878960194963, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.6011281224818695, 0.8174603174603174, 0.5957792207792207, 0.8174603174603174, 0.5982215036378334, 0.8174603174603174, 0.5995165189363417, 0.8174603174603174, 0.5974025974025974, 0.8174603174603174, 0.6006441223832528, 0.8174603174603174, 0.5988700564971752, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5905707196029777, 0.8174603174603174, 0.5957446808510639, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5957446808510639, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5945945945945946, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5957446808510639, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5950819672131148, 0.8174603174603174, 0.5945945945945946, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.5939294503691551, 0.8174603174603174, 0.6028708133971292, 0.816372939169983, 0.6130811662726556, 0.814814814814815, 0.6232339089481946, 0.813287514318442, 0.601755786113328, 0.816372939169983, 0.6130811662726556, 0.814814814814815, 0.6221523959151611, 0.813287514318442, 0.6006389776357827, 0.816372939169983, 0.6130811662726556, 0.814814814814815, 0.6232339089481946, 0.813287514318442, 0.6028708133971292, 0.816372939169983, 0.6130811662726556, 0.814814814814815, 0.6232339089481946, 0.813287514318442, 0.6006389776357827, 0.816372939169983, 0.6130811662726556, 0.814814814814815, 0.6221523959151611, 0.813287514318442, 0.6130811662726556, 0.814814814814815, 0.622848200312989, 0.8119266055045872, 0.622257053291536, 0.8096885813148788, 0.6130811662726556, 0.814814814814815, 0.622848200312989, 0.8119266055045872, 0.622257053291536, 0.8096885813148788, 0.6130811662726556, 0.814814814814815, 0.622848200312989, 0.8112449799196787, 0.622257053291536, 0.8096885813148788, 0.6130811662726556, 0.814814814814815, 0.622848200312989, 0.8119266055045872, 0.622257053291536, 0.8096885813148788, 0.6130811662726556, 0.814814814814815, 0.6223612197028928, 0.8112449799196787, 0.622257053291536, 0.8096885813148788, 0.613564668769716, 0.815489749430524, 0.621069182389937, 0.8119266055045872, 0.6211764705882352, 0.8099078341013826, 0.613564668769716, 0.815489749430524, 0.621069182389937, 0.8119266055045872, 0.6211764705882352, 0.8099078341013826, 0.613564668769716, 0.815489749430524, 0.621069182389937, 0.8119266055045872, 0.6211764705882352, 0.8099078341013826, 0.613564668769716, 0.815489749430524, 0.621069182389937, 0.8119266055045872, 0.6211764705882352, 0.8099078341013826, 0.613564668769716, 0.815489749430524, 0.621069182389937, 0.8119266055045872, 0.6211764705882352, 0.8099078341013826, 0.6073131955484896, 0.8174603174603174, 0.6079365079365078, 0.8161180476730988, 0.6150197628458498, 0.8147727272727273, 0.6073131955484896, 0.8174603174603174, 0.6079365079365078, 0.8161180476730988, 0.6150197628458498, 0.8147727272727273, 0.6084193804606831, 0.8174603174603174, 0.60828025477707, 0.8161180476730988, 0.6150197628458498, 0.8147727272727273, 0.6073131955484896, 0.8174603174603174, 0.6079365079365078, 0.8161180476730988, 0.6150197628458498, 0.8147727272727273, 0.6084193804606831, 0.8174603174603174, 0.6087649402390438, 0.8161180476730988, 0.6150197628458498, 0.8147727272727273, 0.6085578446909667, 0.8167895632444696, 0.6124704025256512, 0.8140989198408185, 0.6138147566718994, 0.8134243458475541, 0.6085578446909667, 0.8167895632444696, 0.6124704025256512, 0.8140989198408185, 0.6138147566718994, 0.8134243458475541, 0.6085578446909667, 0.8167895632444696, 0.6124704025256512, 0.8140989198408185, 0.6138147566718994, 0.8134243458475541, 0.6085578446909667, 0.8167895632444696, 0.6124704025256512, 0.8140989198408185, 0.6138147566718994, 0.8134243458475541, 0.6085578446909667, 0.8167895632444696, 0.6113744075829384, 0.8140989198408185, 0.6153846153846153, 0.8134243458475541, 0.6097946287519747, 0.8167895632444696, 0.6124704025256512, 0.8147727272727273, 0.6124704025256512, 0.8145620022753127, 0.6097946287519747, 0.8167895632444696, 0.6124704025256512, 0.8147727272727273, 0.6124704025256512, 0.8145620022753127, 0.6097946287519747, 0.8167895632444696, 0.6113744075829384, 0.8147727272727273, 0.6124704025256512, 0.8145620022753127, 0.6097946287519747, 0.8167895632444696, 0.6124704025256512, 0.8147727272727273, 0.6124704025256512, 0.8145620022753127, 0.608695652173913, 0.8167895632444696, 0.6113744075829384, 0.8147727272727273, 0.6124704025256512, 0.8145620022753127, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6149019607843137, 0.8092485549132948, 0.601741884402217, 0.8141391106043329, 0.6150234741784036, 0.8092219020172912, 0.6155050900548159, 0.8092485549132948, 0.601741884402217, 0.8141391106043329, 0.6150234741784036, 0.8092219020172912, 0.6155050900548159, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6161063330727131, 0.8092219020172912, 0.6155050900548159, 0.8092485549132948, 0.601741884402217, 0.8141391106043329, 0.6150234741784036, 0.8092219020172912, 0.6155050900548159, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6161063330727131, 0.8092219020172912, 0.615987460815047, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.6022187004754358, 0.8141391106043329, 0.6139389193422083, 0.8092219020172912, 0.6144200626959246, 0.8092485549132948, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8150256118383608, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8150256118383608, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8143507972665147, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8150256118383608, 0.5982636148382005, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5961844197138315, 0.8143507972665147, 0.5942947702060223, 0.8167895632444696, 0.5958795562599049, 0.8165814877910278, 0.5921787709497206, 0.8143507972665147, 0.5942947702060223, 0.8167895632444696, 0.5947660586835845, 0.8165814877910278, 0.5921787709497206, 0.8143507972665147, 0.5942947702060223, 0.8167895632444696, 0.5947660586835845, 0.8165814877910278, 0.5921787709497206, 0.8143507972665147, 0.5942947702060223, 0.8167895632444696, 0.5958795562599049, 0.8165814877910278, 0.5921787709497206, 0.8143507972665147, 0.5942947702060223, 0.8167895632444696, 0.5947660586835845, 0.8165814877910278, 0.5937749401436553, 0.8143507972665147, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8150256118383608, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8150256118383608, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8143507972665147, 0.5976284584980237, 0.8167895632444696, 0.5995241871530531, 0.8165814877910278, 0.5945945945945946, 0.8150256118383608, 0.5982636148382005, 0.8167895632444696, 0.5990491283676703, 0.8165814877910278, 0.5950676213206045, 0.8143507972665147]\n",
      "0.6855225311601151 0.8093023255813954\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer parameters</th>\n",
       "      <th>classifier parameters</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1087</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (1, 1),...</td>\n",
       "      <td>{'solver': 'lbfgs', 'C': 0.5, 'class_weight': ...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>533</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'sag', 'C': 1.5, 'class_weight': None}</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1049</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (1, 1),...</td>\n",
       "      <td>{'solver': 'saga', 'C': 1.5, 'class_weight': N...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>525</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'liblinear', 'C': 1, 'class_weight'...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>527</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'liblinear', 'C': 1.5, 'class_weigh...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1482</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (2, 2),...</td>\n",
       "      <td>{'solver': 'liblinear', 'C': 0.5, 'class_weigh...</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.594295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1488</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (2, 2),...</td>\n",
       "      <td>{'solver': 'sag', 'C': 0.5, 'class_weight': 'b...</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.594295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1476</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (2, 2),...</td>\n",
       "      <td>{'solver': 'lbfgs', 'C': 0.5, 'class_weight': ...</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.594295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1494</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (2, 2),...</td>\n",
       "      <td>{'solver': 'saga', 'C': 0.5, 'class_weight': '...</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.594295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1470</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (2, 2),...</td>\n",
       "      <td>{'solver': 'newton-cg', 'C': 0.5, 'class_weigh...</td>\n",
       "      <td>0.509108</td>\n",
       "      <td>0.594295</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1530 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  vectorizer parameters  ...  f1 score\n",
       "1087  {'analyzer': 'char_wb', 'ngram_range': (1, 1),...  ...  0.817460\n",
       "533   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.817460\n",
       "1049  {'analyzer': 'char_wb', 'ngram_range': (1, 1),...  ...  0.817460\n",
       "525   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.817460\n",
       "527   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.817460\n",
       "...                                                 ...  ...       ...\n",
       "1482  {'analyzer': 'char_wb', 'ngram_range': (2, 2),...  ...  0.594295\n",
       "1488  {'analyzer': 'char_wb', 'ngram_range': (2, 2),...  ...  0.594295\n",
       "1476  {'analyzer': 'char_wb', 'ngram_range': (2, 2),...  ...  0.594295\n",
       "1494  {'analyzer': 'char_wb', 'ngram_range': (2, 2),...  ...  0.594295\n",
       "1470  {'analyzer': 'char_wb', 'ngram_range': (2, 2),...  ...  0.594295\n",
       "\n",
       "[1530 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### Step 2: get the output files saved and calculate the evaluation metrics of all of them\n",
    "\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html \n",
    "analyzers = ['word', 'char', 'char_wb'] # default is word\n",
    "ngram_ranges = [(1,1), (1, 2), (2,2)] #unigram or bigram  / bigram only /(default is unigram) (1,1)\n",
    "\n",
    "# https://stackoverflow.com/questions/27697766/understanding-min-df-and-max-df-in-scikit-countvectorizer\n",
    "min_dfs = [0.01, 0.05 ,1.0 ] #  default=1 (does not ignore), ignore terms that appear in less than 1% of the documents\". % too infrequenct\n",
    "max_dfs = [1.0, 0.8 , 0.9 ] # default=1.0 (does not ignore), ignore terms that appear in more than 80% of the documents\" % too frequent\n",
    "\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html \n",
    "solvers = ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'] # , default=’lbfgs’\n",
    "C_values =[0.5 ,1, 1.5]  # default=1.0\n",
    "class_weights = ['balanced', None] # default=None\n",
    "\n",
    "\n",
    "accuracy_scores = []\n",
    "f1_scores = []\n",
    "\n",
    "vect_parameters = []\n",
    "clsf_parameters = []\n",
    "\n",
    "for analyzer in analyzers:\n",
    "  for ngram_range in ngram_ranges:\n",
    "    for min_df in min_dfs:\n",
    "      for max_df in max_dfs:\n",
    "          if max_df <= min_df: # we remove impossible combinations\n",
    "            pass\n",
    "          else:\n",
    "            for solver in solvers: \n",
    "              for c in C_values:\n",
    "                for class_weight in class_weights:\n",
    "                   vec_meta_parameters_name = '_analyzer='+str(analyzer)+'_ngram_range='+str(ngram_range)+'_max_df='+str(max_df)+'_min_df='+str(min_df)\n",
    "                   clf_meta_parameters_name = '_C='+str(c)+'_class_weight='+ str(class_weight)+'_solver='+str(solver)\n",
    "                   filename = COLA_output_folder+'LogisticRegression_classifier'+clf_meta_parameters_name+'_'+'tf-idf_vectorizer'+vec_meta_parameters_name+\".out\"\n",
    "                   if os.path.isfile(filename): #is file exists\n",
    "                      #open and read picle file to get predictions \n",
    "                      [ _ , y_LR_t ] = pickle.load( open(filename, 'rb'))\n",
    "\n",
    "                      #compare and evaluate the predictions\n",
    "                      #print(filename)\n",
    "                      #print(classification_report(dev_cola_data['Acceptability judgment label'], y_LR))\n",
    "\n",
    "                      vect_params={'analyzer':analyzer, 'ngram_range': ngram_range, 'min_df':min_df, 'max_df':max_df}\n",
    "                      clsf_params={'solver':solver, 'C':c, 'class_weight':class_weight}\n",
    "\n",
    "                      accuracy_scores.append( accuracy_score(dev_cola_data['Acceptability judgment label'], y_LR_t)  )\n",
    "                      f1_scores.append( f1_score(dev_cola_data['Acceptability judgment label'],y_LR_t) )\n",
    "\n",
    "                      vect_parameters.append(vect_params)\n",
    "                      clsf_parameters.append(clsf_params)\n",
    "\n",
    "\n",
    "print(accuracy_scores)\n",
    "print(f1_scores)\n",
    "print(baseline_accuracy, baseline_f1_score)\n",
    "\n",
    "d =  {'vectorizer parameters': vect_parameters, 'classifier parameters': clsf_parameters, 'accuracy score': accuracy_scores, 'f1 score' : f1_scores}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.sort_values('accuracy score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 135,
     "status": "ok",
     "timestamp": 1635565149453,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "LsWLFAkgRHHq",
    "outputId": "078a2877-ba08-4555-94b7-97cb9b0f0a13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Improvement of 0.84% in accuracy compared to the baseline.\n"
     ]
    }
   ],
   "source": [
    "best_accuracy_score=df.sort_values('accuracy score', ascending=False)['accuracy score'].iloc[0]\n",
    "\n",
    "print('Best Improvement = {:0.2f}% in accuracy compared to the baseline.'.format( 100 * (best_accuracy_score - baseline_accuracy) / baseline_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 133,
     "status": "ok",
     "timestamp": 1635565164480,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "BUeje4qSPIR8",
    "outputId": "8ddd2240-84f4-4e08-8af8-e7ee7780c006"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>vectorizer parameters</th>\n",
       "      <th>classifier parameters</th>\n",
       "      <th>accuracy score</th>\n",
       "      <th>f1 score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1061</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (1, 1),...</td>\n",
       "      <td>{'solver': 'lbfgs', 'C': 1.5, 'class_weight': ...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1019</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (1, 1),...</td>\n",
       "      <td>{'solver': 'saga', 'C': 1.5, 'class_weight': N...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1021</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (1, 1),...</td>\n",
       "      <td>{'solver': 'newton-cg', 'C': 0.5, 'class_weigh...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>{'analyzer': 'word', 'ngram_range': (1, 2), 'm...</td>\n",
       "      <td>{'solver': 'liblinear', 'C': 1.5, 'class_weigh...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1023</th>\n",
       "      <td>{'analyzer': 'char_wb', 'ngram_range': (1, 1),...</td>\n",
       "      <td>{'solver': 'newton-cg', 'C': 1, 'class_weight'...</td>\n",
       "      <td>0.691275</td>\n",
       "      <td>0.817460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>588</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'sag', 'C': 0.5, 'class_weight': 'b...</td>\n",
       "      <td>0.525407</td>\n",
       "      <td>0.590571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>594</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'saga', 'C': 0.5, 'class_weight': '...</td>\n",
       "      <td>0.525407</td>\n",
       "      <td>0.590571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>592</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'sag', 'C': 1.5, 'class_weight': 'b...</td>\n",
       "      <td>0.525407</td>\n",
       "      <td>0.590571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'lbfgs', 'C': 0.5, 'class_weight': ...</td>\n",
       "      <td>0.525407</td>\n",
       "      <td>0.590571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>596</th>\n",
       "      <td>{'analyzer': 'char', 'ngram_range': (1, 1), 'm...</td>\n",
       "      <td>{'solver': 'saga', 'C': 1, 'class_weight': 'ba...</td>\n",
       "      <td>0.523490</td>\n",
       "      <td>0.590272</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1530 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  vectorizer parameters  ...  f1 score\n",
       "1061  {'analyzer': 'char_wb', 'ngram_range': (1, 1),...  ...  0.817460\n",
       "1019  {'analyzer': 'char_wb', 'ngram_range': (1, 1),...  ...  0.817460\n",
       "1021  {'analyzer': 'char_wb', 'ngram_range': (1, 1),...  ...  0.817460\n",
       "347   {'analyzer': 'word', 'ngram_range': (1, 2), 'm...  ...  0.817460\n",
       "1023  {'analyzer': 'char_wb', 'ngram_range': (1, 1),...  ...  0.817460\n",
       "...                                                 ...  ...       ...\n",
       "588   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.590571\n",
       "594   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.590571\n",
       "592   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.590571\n",
       "576   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.590571\n",
       "596   {'analyzer': 'char', 'ngram_range': (1, 1), 'm...  ...  0.590272\n",
       "\n",
       "[1530 rows x 4 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_values('f1 score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1635565361620,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "bFWeqXq2Q5dT",
    "outputId": "237cc1a9-0a0c-4273-d8b0-3e90a3c35dc8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Improvement = 1.01% in f1 score compared to the baseline.\n"
     ]
    }
   ],
   "source": [
    "best_f1_score=df.sort_values('f1 score', ascending=False)['f1 score'].iloc[0]\n",
    "\n",
    "print('Best Improvement = {:0.2f}% in f1 score compared to the baseline.'.format( 100 * (best_f1_score - baseline_f1_score) / baseline_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_brj_iK6SSjk"
   },
   "outputs": [],
   "source": [
    "#maybe to investigate results more"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vrlWnNdLAMGT"
   },
   "source": [
    "#### Impact of  pre-processing techniques (todo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v7RtUXevAUUd"
   },
   "outputs": [],
   "source": [
    "we do not consider removing stop words -> makes them wrong "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4aMFpamQAUtj"
   },
   "source": [
    "#### Performace of other feature- models  - Random Forest + TF-IDF VECTORIZER ---> made accuracy + f1 score worst / then using Grid search, minor imporvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 202689,
     "status": "ok",
     "timestamp": 1635566084371,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "Nen8f8vSAshE",
    "outputId": "81e9658d-96ac-4095-f39d-428c92d8aca9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.15      0.22       322\n",
      "           1       0.71      0.91      0.80       721\n",
      "\n",
      "    accuracy                           0.68      1043\n",
      "   macro avg       0.57      0.53      0.51      1043\n",
      "weighted avg       0.62      0.68      0.62      1043\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://www.kaggle.com/onadegibert/sentiment-analysis-with-tfidf-and-random-forest\n",
    "from sklearn.ensemble import RandomForestClassifier \n",
    "import numpy as np\n",
    "\n",
    "def vectorize(data,tfidf_vect_fit):\n",
    "    X_tfidf = tfidf_vect_fit.transform(data)\n",
    "    words = tfidf_vect_fit.get_feature_names()\n",
    "    X_tfidf_df = pd.DataFrame(X_tfidf.toarray())\n",
    "    X_tfidf_df.columns = words\n",
    "    return(X_tfidf_df)\n",
    "\n",
    "tfidf_vect = TfidfVectorizer()\n",
    "tfidf_vect_fit=tfidf_vect.fit(train_cola_data['Sentence'])\n",
    "X_train=vectorize(train_cola_data['Sentence'],tfidf_vect_fit)\n",
    "\n",
    "clf_RF = RandomForestClassifier()\n",
    "clf_RF.fit(X_train, train_cola_data['Acceptability judgment label'])\n",
    "\n",
    "X_test=vectorize(dev_cola_data['Sentence'],tfidf_vect_fit)\n",
    "y_RF = clf_RF.predict(X_test) # run the classifier\n",
    "print(classification_report(dev_cola_data['Acceptability judgment label'], y_RF))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bxP5njKjbCTe"
   },
   "outputs": [],
   "source": [
    "#todo: save the RF model and the ouputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 147,
     "status": "ok",
     "timestamp": 1635566752644,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "bwxhudazUtI7",
    "outputId": "2f5560d2-2e36-4c67-ba63-132773bc3f7c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6855225311601151 0.8093023255813954\n",
      "0.675934803451582 0.7951515151515153\n",
      "Improvement of -1.40% in accuracy compared to the baseline.\n",
      "Improvement of -1.75% in f1_score compared to the baseline.\n"
     ]
    }
   ],
   "source": [
    "#compare to the baseline\n",
    "RF_accuracy = accuracy_score(dev_cola_data['Acceptability judgment label'], y_RF)\n",
    "RF_f1_score = f1_score(dev_cola_data['Acceptability judgment label'], y_RF)\n",
    "\n",
    "print(baseline_accuracy, baseline_f1_score)\n",
    "print(RF_accuracy,RF_f1_score )\n",
    "\n",
    "print('Improvement of {:0.2f}% in accuracy compared to the baseline.'.format( 100 * (RF_accuracy - baseline_accuracy) / baseline_accuracy))\n",
    "print('Improvement of {:0.2f}% in f1_score compared to the baseline.'.format( 100 * (RF_f1_score - baseline_f1_score) / baseline_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 376
    },
    "executionInfo": {
     "elapsed": 101017,
     "status": "ok",
     "timestamp": 1635567152536,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "Z7VDLDuROohv",
    "outputId": "0cb13077-37b3-48bf-d700-d29a2654535d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': [0.5, 1, 1.5], 'class_weight': ['balanced', None], 'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga']}\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>params</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 10,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 200, 'min_samples_split': 8, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 1000, 'min_samples_split': 12...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 300, 'min_samples_split': 12,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.704362</td>\n",
       "      <td>0.000462</td>\n",
       "      <td>{'n_estimators': 100, 'min_samples_split': 8, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_test_score  ...                                             params\n",
       "0         0.704362  ...  {'n_estimators': 300, 'min_samples_split': 8, ...\n",
       "1         0.704362  ...  {'n_estimators': 300, 'min_samples_split': 12,...\n",
       "2         0.704362  ...  {'n_estimators': 300, 'min_samples_split': 10,...\n",
       "3         0.704362  ...  {'n_estimators': 100, 'min_samples_split': 8, ...\n",
       "4         0.704362  ...  {'n_estimators': 200, 'min_samples_split': 8, ...\n",
       "5         0.704362  ...  {'n_estimators': 100, 'min_samples_split': 12,...\n",
       "6         0.704362  ...  {'n_estimators': 300, 'min_samples_split': 12,...\n",
       "7         0.704362  ...  {'n_estimators': 1000, 'min_samples_split': 12...\n",
       "8         0.704362  ...  {'n_estimators': 300, 'min_samples_split': 12,...\n",
       "9         0.704362  ...  {'n_estimators': 100, 'min_samples_split': 8, ...\n",
       "\n",
       "[10 rows x 3 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# improve Random Forest using Grid search\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "max_depths = [80, 90, 100, 110]\n",
    "max_features = [2,3]\n",
    "min_samples_leafs= [3, 4, 5]\n",
    "min_samples_splits =  [8, 10, 12]\n",
    "n_estimators = [100, 200, 300, 1000]\n",
    "bootstrap = [True]\n",
    "\n",
    "param_dist = dict(n_estimators=n_estimators, bootstrap=bootstrap, max_depth=max_depths, max_features=max_features, min_samples_leaf = min_samples_leafs , min_samples_split = min_samples_splits )\n",
    "print(param_grid)\n",
    "\n",
    "rand = RandomizedSearchCV(clf_RF, param_dist, cv=10, n_iter=10, random_state=5)\n",
    "rand.fit(X_train, train_cola_data['Acceptability judgment label'])\n",
    "pd.DataFrame(rand.cv_results_)[['mean_test_score', 'std_test_score', 'params']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 154,
     "status": "ok",
     "timestamp": 1635567278355,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "m6usUga3apA6",
    "outputId": "d3d263bb-6a01-439f-ae84-aed73959f573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7043620538886156\n",
      "{'n_estimators': 300, 'min_samples_split': 8, 'min_samples_leaf': 5, 'max_features': 3, 'max_depth': 80, 'bootstrap': True}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00       322\n",
      "           1       0.69      1.00      0.82       721\n",
      "\n",
      "    accuracy                           0.69      1043\n",
      "   macro avg       0.35      0.50      0.41      1043\n",
      "weighted avg       0.48      0.69      0.57      1043\n",
      "\n",
      "0.6912751677852349 0.8174603174603174\n",
      "0.6855225311601151 0.8093023255813954\n",
      "Improvement of 0.84% in accuracy compared to the baseline.\n",
      "Improvement of 1.01% in f1_score compared to the baseline.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "# examine the best model\n",
    "print(rand.best_score_)\n",
    "print(rand.best_params_)\n",
    "\n",
    "# predict with the best parameteres \n",
    "y_RF_best = rand.predict(X_test) \n",
    "print(classification_report(dev_cola_data['Acceptability judgment label'], y_RF_best))\n",
    "\n",
    "RF_best_accuracy = accuracy_score(dev_cola_data['Acceptability judgment label'], y_RF_best)\n",
    "RF_best_f1_score = f1_score(dev_cola_data['Acceptability judgment label'], y_RF_best)\n",
    "\n",
    "#compare to the baseline\n",
    "print(RF_best_accuracy, RF_best_f1_score)\n",
    "print(baseline_accuracy, baseline_f1_score)\n",
    "\n",
    "print('Improvement of {:0.2f}% in accuracy compared to the baseline.'.format( 100 * (RF_best_accuracy - baseline_accuracy) / baseline_accuracy))\n",
    "print('Improvement of {:0.2f}% in f1_score compared to the baseline.'.format( 100 * (RF_best_f1_score - baseline_f1_score) / baseline_f1_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jTnOQeTXfxOn"
   },
   "outputs": [],
   "source": [
    "#todo: save the best RF-model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tNQuumVoAjsl"
   },
   "source": [
    "#### Performance of deep-learning models using transfer learning. BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OF-_9Fzy3EAQ"
   },
   "outputs": [],
   "source": [
    "!pip install fastai --upgrade \n",
    "#!pip install fastai2\n",
    "!pip uninstall fastai2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 126,
     "status": "ok",
     "timestamp": 1635637194635,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "ns-msslQxvK0",
    "outputId": "a16047ce-f8fd-4a91-8954-e51d8e402e70"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: Tesla K80\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "\n",
    "    # Tell PyTorch to use the GPU.    \n",
    "    device = torch.device(\"cuda\")\n",
    "\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 230
    },
    "executionInfo": {
     "elapsed": 15940,
     "status": "ok",
     "timestamp": 1635637212981,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "tj1-GBHr3c9m",
    "outputId": "36566cb0-6a52-4dcd-bedf-87e1c98641ee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Path('Data/CoLA/dev.tsv'), Path('Data/CoLA/train.tsv'), Path('Data/CoLA/test.tsv')]\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj everybody who has ever , worked in any office which contained any xxunk which had ever been used to type any letters which had to be signed by any xxunk who ever worked in any department like mine will know what i xxunk .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj xxunk plays the guitar and finds xxunk for all the old folk songs which are still sung in these hills , and xxmaj xxunk writes down all the old folk songs which are still sung in these hills .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj handsome though everyone xxunk me to try to force xxmaj bill to make xxmaj xxunk agree that xxmaj dick is , xxmaj i 'm still going to xxmaj marry xxmaj herman .</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from fastai.text.all import *\n",
    "#from fastai2.imports import *\n",
    "\n",
    "files = get_files(COLA_data_folder, recurse=False, extensions='.tsv')\n",
    "print(files)\n",
    "\n",
    "dls = TextDataLoaders.from_df(train_cola_data, text_col='Sentence', label_col='Acceptability judgment label', valid_pct=0.1)  ## Random 10% as validation\n",
    "\n",
    "dls.show_batch(max_n=3) # tokenized text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 148,
     "status": "ok",
     "timestamp": 1635633083163,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "AQufJpnIU6QC",
    "outputId": "7b76bcad-af64-4339-e2ce-06c60297b7d3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7696, 855)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dls.train_ds), len(dls.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 623,
     "status": "ok",
     "timestamp": 1635633890000,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "yCVOrdEK3YBm",
    "outputId": "83fc8639-befe-4f38-e67f-86bd0d022770"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SequentialRNN(\n",
       "  (0): SentenceEncoder(\n",
       "    (module): AWD_LSTM(\n",
       "      (encoder): Embedding(2624, 400, padding_idx=1)\n",
       "      (encoder_dp): EmbeddingDropout(\n",
       "        (emb): Embedding(2624, 400, padding_idx=1)\n",
       "      )\n",
       "      (rnns): ModuleList(\n",
       "        (0): WeightDropout(\n",
       "          (module): LSTM(400, 1152, batch_first=True)\n",
       "        )\n",
       "        (1): WeightDropout(\n",
       "          (module): LSTM(1152, 1152, batch_first=True)\n",
       "        )\n",
       "        (2): WeightDropout(\n",
       "          (module): LSTM(1152, 400, batch_first=True)\n",
       "        )\n",
       "      )\n",
       "      (input_dp): RNNDropout()\n",
       "      (hidden_dps): ModuleList(\n",
       "        (0): RNNDropout()\n",
       "        (1): RNNDropout()\n",
       "        (2): RNNDropout()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (1): PoolingLinearClassifier(\n",
       "    (layers): Sequential(\n",
       "      (0): LinBnDrop(\n",
       "        (0): BatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.2, inplace=False)\n",
       "        (2): Linear(in_features=1200, out_features=50, bias=False)\n",
       "        (3): ReLU(inplace=True)\n",
       "      )\n",
       "      (1): LinBnDrop(\n",
       "        (0): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (1): Dropout(p=0.1, inplace=False)\n",
       "        (2): Linear(in_features=50, out_features=2, bias=False)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a pretrained model\n",
    "learn = text_classifier_learner(dls, AWD_LSTM, drop_mult=0.5, metrics=[accuracy, perplexity] )\n",
    "learn.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "executionInfo": {
     "elapsed": 736,
     "status": "ok",
     "timestamp": 1635633991819,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "skkKWtNK-G4Y",
    "outputId": "069c3ea0-86e6-4825-e4c0-7d9d5c4f879a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', tensor(0), tensor([0.5068, 0.4932]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', tensor(0), tensor([0.5026, 0.4974]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', tensor(0), tensor([0.5100, 0.4900]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', tensor(1), tensor([0.4963, 0.5037]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('0', tensor(0), tensor([0.5006, 0.4994]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj will put a picture of xxmaj bill on your desk before tomorrow , this girl in the red coat will put a picture of xxmaj bill on your desk before tomorrow .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj smith loaned a valuable collection of manuscripts to the library , and his widow later donated a valuable collection of manuscripts to the library .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj ron wanted to wear a tuxedo to the party , but wear a tuxedo to the party xxmaj caspar could n't decide whether to .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj the students of xxmaj english from xxmaj seoul xxunk many issues in the process of xxunk , xxunk , and xxunk the poems .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj this girl in the red coat will eat her breakfast and will put a picture of xxmaj bill on your desk before tomorrow .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj this is the sort of problem which the sooner you solve the more easily you 'll satisfy the folks up at corporate headquarters .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj that piece of ice is too big for it to be likely for him to be able to pick up with a xxunk .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj this $ 500 xxunk will cost the government $ 500 , xxrep 3 0 to prove that xxmaj senator xxmaj jones accepted .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos the book with a red cover of poems from xxmaj blackwell by xxmaj robert xxmaj burns takes a very long time to read .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test it\n",
    "print(learn.predict(\"I really liked that movie!\"))\n",
    "print(learn.predict(\"I hate  that movie!\"))\n",
    "print(learn.predict(\"I hate.\"))\n",
    "print(learn.predict(\"himself we\")) \n",
    "print(learn.predict(\"i we\")) \n",
    "learn.show_results() # on the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 92684,
     "status": "ok",
     "timestamp": 1635634091458,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "TG6vkxso2Drp",
    "outputId": "1bf71db4-d531-4dc0-8ae4-bb71c3ca84eb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.660408</td>\n",
       "      <td>0.617509</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>1.854303</td>\n",
       "      <td>00:16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>perplexity</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.608951</td>\n",
       "      <td>0.603677</td>\n",
       "      <td>0.700585</td>\n",
       "      <td>1.828831</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.580884</td>\n",
       "      <td>0.580078</td>\n",
       "      <td>0.707602</td>\n",
       "      <td>1.786177</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.542218</td>\n",
       "      <td>0.567561</td>\n",
       "      <td>0.719298</td>\n",
       "      <td>1.763959</td>\n",
       "      <td>00:18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.467514</td>\n",
       "      <td>0.583688</td>\n",
       "      <td>0.728655</td>\n",
       "      <td>1.792638</td>\n",
       "      <td>00:19</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# then fine-tune it\n",
    "learn.fine_tune(4, 1e-2)   # 4 epochs\n",
    "# the accuracy shown is on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 413
    },
    "executionInfo": {
     "elapsed": 719,
     "status": "ok",
     "timestamp": 1635634097270,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "rjxTIgyC_sfT",
    "outputId": "0115a502-81b9-40f3-e077-a54b64b52fcb"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', tensor(1), tensor([0.0607, 0.9393]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', tensor(1), tensor([0.0194, 0.9806]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', tensor(1), tensor([0.1862, 0.8138]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', tensor(1), tensor([0.3327, 0.6673]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('1', tensor(1), tensor([0.3626, 0.6374]))\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>category_</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>xxbos xxmaj will put a picture of xxmaj bill on your desk before tomorrow , this girl in the red coat will put a picture of xxmaj bill on your desk before tomorrow .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>xxbos xxmaj smith loaned a valuable collection of manuscripts to the library , and his widow later donated a valuable collection of manuscripts to the library .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>xxbos xxmaj ron wanted to wear a tuxedo to the party , but wear a tuxedo to the party xxmaj caspar could n't decide whether to .</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>xxbos xxmaj the students of xxmaj english from xxmaj seoul xxunk many issues in the process of xxunk , xxunk , and xxunk the poems .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>xxbos xxmaj this is the sort of problem which the sooner you solve the more easily you 'll satisfy the folks up at corporate headquarters .</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>xxbos xxmaj this girl in the red coat will eat her breakfast and will put a picture of xxmaj bill on your desk before tomorrow .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>xxbos xxmaj that piece of ice is too big for it to be likely for him to be able to pick up with a xxunk .</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>xxbos xxmaj pat wanted to try to go to xxmaj xxunk , and xxmaj chris to go to xxmaj rome . to xxmaj rome .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>xxbos xxmaj the xxunk to which the xxunk took objection had to do with the mixed xxunk of a woman and a xxunk xxunk .</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# test it\n",
    "print(learn.predict(\"I really liked that movie!\"))\n",
    "print(learn.predict(\"I hate  that movie!\"))\n",
    "print(learn.predict(\"I hate.\"))\n",
    "print(learn.predict(\"himself we\")) \n",
    "print(learn.predict(\"i we\")) \n",
    "\n",
    "learn.show_results() # on the training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 782
    },
    "executionInfo": {
     "elapsed": 2025,
     "status": "ok",
     "timestamp": 1635634433761,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "BC9-IVZ0Wd6f",
    "outputId": "c806d6bc-ca89-4748-ba3c-5e054dfae9b9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[0.3026, 0.6974],\n",
       "         [0.0828, 0.9172],\n",
       "         [0.3063, 0.6937],\n",
       "         ...,\n",
       "         [0.4221, 0.5779],\n",
       "         [0.2626, 0.7374],\n",
       "         [0.0614, 0.9386]]),\n",
       " TensorCategory([1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "         0, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 0,\n",
       "         1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0,\n",
       "         1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0,\n",
       "         0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1,\n",
       "         1, 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1,\n",
       "         0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1,\n",
       "         1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "         1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1,\n",
       "         0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1,\n",
       "         0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "         1, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 0, 1, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0,\n",
       "         1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1,\n",
       "         1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
       "         0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1,\n",
       "         1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "         1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "         0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1]),\n",
       " 855,\n",
       " 855)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds, y = learn.get_preds(dl = dls.valid)\n",
    "preds, y,  len(preds), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 128,
     "status": "ok",
     "timestamp": 1635634454517,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "V8iZJzfBXJXv",
    "outputId": "a6f3ae3c-01aa-4862-ead2-170276e20924"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorBase(0.7287)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(preds, y) # on the validation set "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "executionInfo": {
     "elapsed": 2581,
     "status": "ok",
     "timestamp": 1635635363558,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "5P5KGFisHwqq",
    "outputId": "61d11ac5-533f-4660-d081-f7d0c915b76d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1043, 0)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dls_test = TextDataLoaders.from_df(dev_cola_data, text_col='Sentence', label_col='Acceptability judgment label', valid_pct=0.0)  # no validation\n",
    "len(dls_test.train_ds), len(dls_test.valid_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 377
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "error",
     "timestamp": 1635637697569,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "JX1aUhaRbutI",
    "outputId": "d9f71938-9044-4d81-ad0b-f7f6163dbc25"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "        <style>\n",
       "            /* Turns off some styling */\n",
       "            progress {\n",
       "                /* gets rid of default border in Firefox and Opera. */\n",
       "                border: none;\n",
       "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "                background-size: auto;\n",
       "            }\n",
       "            .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "                background: #F44336;\n",
       "            }\n",
       "        </style>\n",
       "      <progress value='0' class='' max='1043' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1043 00:00<00:00]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-82-2d058c3adba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#b = torch.from_numpy(a)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#preds_test, y_test  = learn.get_preds(DatasetType.Test)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mpreds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_preds\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdls_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_ds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwith_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mget_preds\u001b[0;34m(self, ds_idx, dl, with_input, with_decoded, with_loss, act, inner, reorder, cbs, **kwargs)\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwith_loss\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mctx_mgrs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_not_reduced\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mContextManagers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx_mgrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 253\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_epoch_validate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mact\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mact\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'activation'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_tensors\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_epoch_validate\u001b[0;34m(self, ds_idx, dl)\u001b[0m\n\u001b[1;32m    201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdls\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mds_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall_batches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'validate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelValidException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mall_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mall_batches\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mone_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36mone_batch\u001b[0;34m(self, i, b)\u001b[0m\n\u001b[1;32m    192\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 194\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_one_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'batch'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCancelBatchException\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_epoch_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_with_events\u001b[0;34m(self, f, event_type, ex, final)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_with_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfinal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnoop\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'before_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mex\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_cancel_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'after_{event_type}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m  \u001b[0mfinal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/learner.py\u001b[0m in \u001b[0;36m_do_one_batch\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_do_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'after_pred'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0myb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/fastai/text/models/core.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mbs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "#b = torch.from_numpy(a)\n",
    "#preds_test, y_test  = learn.get_preds(DatasetType.Test)\n",
    "preds_test, y_test = learn.get_preds(dl = dls_test.train_ds, with_loss=True)\n",
    "accuracy(preds_test, y_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LiCeWCPgAjUN"
   },
   "outputs": [],
   "source": [
    "# https://www.kaggle.com/hassanamin/bert-pytorch-cola-classification : BERT for Cola\n",
    "# also the other notebook does transfer learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JXMVVRvfxnfl"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CVN1Ex3BThka"
   },
   "outputs": [],
   "source": [
    "\n",
    "probably BERT will give best result --> so look at chaging hyparmeters here too, for best possible \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A3rZklEFUw4C"
   },
   "source": [
    "### Step4: Choosing the best system and its configuration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pjp9O_CoU96L"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xO-UYX-3U_B2"
   },
   "source": [
    "### Step5: Neat Analysis of the chosen system ( including creating new data ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kqchptpxVMrK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "opB6zJHqVX5F"
   },
   "source": [
    "### Step6: Improving the chosen system (based on the analysis above)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UZZEDkibVhMy"
   },
   "outputs": [],
   "source": [
    "# todo : save the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EjEEwhtWVk9q"
   },
   "source": [
    "### Step7 : Testing the system on new test data (later)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2zzaMgSpVunq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V1ugpvFcTAL6"
   },
   "source": [
    "## Task 2 : MSRPC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: seaborn in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (0.11.2)\n",
      "Collecting editdistance\n",
      "  Downloading editdistance-0.6.0-cp38-cp38-win_amd64.whl (24 kB)\n",
      "Requirement already satisfied: pandas>=0.23 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: matplotlib>=2.2 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from seaborn) (1.7.1)\n",
      "Requirement already satisfied: numpy>=1.15 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from seaborn) (1.21.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from matplotlib>=2.2->seaborn) (8.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from matplotlib>=2.2->seaborn) (2.4.7)\n",
      "Requirement already satisfied: pytz>=2017.3 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\kacem\\anaconda3\\envs\\nlp-project-env-conda\\lib\\site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: editdistance\n",
      "Successfully installed editdistance-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip install seaborn editdistance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jDNJj5ekaoX2"
   },
   "source": [
    "### Step1 : Download and get to Know the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "CommandNotFoundError: Your shell has not been properly configured to use 'conda activate'.\n",
      "To initialize your shell, run\n",
      "\n",
      "    $ conda init <SHELL_NAME>\n",
      "\n",
      "Currently supported shells are:\n",
      "  - bash\n",
      "  - fish\n",
      "  - tcsh\n",
      "  - xonsh\n",
      "  - zsh\n",
      "  - powershell\n",
      "\n",
      "See 'conda init --help' for more information and options.\n",
      "\n",
      "IMPORTANT: You may need to close and restart your shell after running 'conda init'.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda activate nlp-project-env-conda\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'TypeAlias' from 'typing_extensions' (/tools/anaconda3/lib/python3.8/site-packages/typing_extensions.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-a27a9459efc7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mMSRP\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"glue\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mrpc\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/datasets/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_reader\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowReader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mReadInstruction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamBasedBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBuilderConfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDatasetBuilder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGeneratorBasedBuilder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mcombine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0minterleave_datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/datasets/builder.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     42\u001b[0m )\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0marrow_writer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mArrowWriter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBeamWriter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata_files\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFilesDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_sanitize_patterns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdataset_dict\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mIterableDatasetDict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfingerprint\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHasher\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/datasets/data_files.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTuple\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcurrent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthread_map\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/huggingface_hub/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mwhoami\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m )\n\u001b[0;32m---> 55\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhub_mixin\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mModelHubMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPyTorchModelHubMixin\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0minference_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInferenceApi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m from .keras_mixin import (\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/huggingface_hub/hub_mixin.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mfile_download\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhf_hub_download\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_torch_available\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhf_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mrepository\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRepository\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/huggingface_hub/repository.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mREPO_TYPES_URL_PREFIXES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mREPOCARD_NAME\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mhuggingface_hub\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepocard\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetadata_load\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetadata_save\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhf_api\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mENDPOINT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfApi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mHfFolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrepo_type_and_id_from_hf_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/huggingface_hub/repocard.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m from huggingface_hub.repocard_types import (\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mModelIndex\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mSingleMetric\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tools/anaconda3/lib/python3.8/site-packages/huggingface_hub/repocard_types.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping_extensions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTypeAlias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'TypeAlias' from 'typing_extensions' (/tools/anaconda3/lib/python3.8/site-packages/typing_extensions.py)"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "\n",
    "MSRP = datasets.load_dataset(\"glue\", \"mrpc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "l6Bh1XMAlzI3"
   },
   "outputs": [],
   "source": [
    "MSRPC_data_folder = 'Data/MSRPC/'\n",
    "MSRPC_models_folder  = 'Models/MSRPC/'\n",
    "MSRPC_output_folder = 'Outputs/MSRPC/'\n",
    "\n",
    "train_MSRPC_file = MSRPC_data_folder +'msr_paraphrase_train.txt'\n",
    "test_MSRPC_file =MSRPC_data_folder  + 'msr_paraphrase_test.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "executionInfo": {
     "elapsed": 335,
     "status": "ok",
     "timestamp": 1635567867698,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "a17JvnXFl1kJ",
    "outputId": "4b131364-5363-4c2e-e68c-c9e161991b9c"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Quality Score</th>\n",
       "      <th>Sentence 1</th>\n",
       "      <th>Sentence 2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Amrozi accused his brother, whom he called \"th...</td>\n",
       "      <td>Referring to him as only \"the witness\", Amrozi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Yucaipa owned Dominick's before selling the ch...</td>\n",
       "      <td>Yucaipa bought Dominick's in 1995 for $693 mil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>They had published an advertisement on the Int...</td>\n",
       "      <td>On June 10, the ship's owners had published an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>Around 0335 GMT, Tab shares were up 19 cents, ...</td>\n",
       "      <td>Tab shares jumped 20 cents, or 4.6%, to set a ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>The stock rose $2.11, or about 11 percent, to ...</td>\n",
       "      <td>PG&amp;E Corp. shares jumped $1.63 or 8 percent to...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3957</th>\n",
       "      <td>1</td>\n",
       "      <td>At this point, Mr. Brando announced: 'Somebody...</td>\n",
       "      <td>Brando said that \"somebody ought to put a bull...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>0</td>\n",
       "      <td>Martin, 58, will be freed today after serving ...</td>\n",
       "      <td>Martin served two thirds of a five-year senten...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3959</th>\n",
       "      <td>1</td>\n",
       "      <td>We have concluded that the outlook for price s...</td>\n",
       "      <td>In a statement, the ECB said the outlook for p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3960</th>\n",
       "      <td>1</td>\n",
       "      <td>The notification was first reported Friday by ...</td>\n",
       "      <td>MSNBC.com first reported the CIA request on Fr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3961</th>\n",
       "      <td>0</td>\n",
       "      <td>The 30-year bond US30YT=RR rose 22/32 for a yi...</td>\n",
       "      <td>The 30-year bond US30YT=RR grew 1-3/32 for a y...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3962 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Quality Score                                         Sentence 1  \\\n",
       "0                 1  Amrozi accused his brother, whom he called \"th...   \n",
       "1                 0  Yucaipa owned Dominick's before selling the ch...   \n",
       "2                 1  They had published an advertisement on the Int...   \n",
       "3                 0  Around 0335 GMT, Tab shares were up 19 cents, ...   \n",
       "4                 1  The stock rose $2.11, or about 11 percent, to ...   \n",
       "...             ...                                                ...   \n",
       "3957              1  At this point, Mr. Brando announced: 'Somebody...   \n",
       "3958              0  Martin, 58, will be freed today after serving ...   \n",
       "3959              1  We have concluded that the outlook for price s...   \n",
       "3960              1  The notification was first reported Friday by ...   \n",
       "3961              0  The 30-year bond US30YT=RR rose 22/32 for a yi...   \n",
       "\n",
       "                                             Sentence 2  \n",
       "0     Referring to him as only \"the witness\", Amrozi...  \n",
       "1     Yucaipa bought Dominick's in 1995 for $693 mil...  \n",
       "2     On June 10, the ship's owners had published an...  \n",
       "3     Tab shares jumped 20 cents, or 4.6%, to set a ...  \n",
       "4     PG&E Corp. shares jumped $1.63 or 8 percent to...  \n",
       "...                                                 ...  \n",
       "3957  Brando said that \"somebody ought to put a bull...  \n",
       "3958  Martin served two thirds of a five-year senten...  \n",
       "3959  In a statement, the ECB said the outlook for p...  \n",
       "3960  MSNBC.com first reported the CIA request on Fr...  \n",
       "3961  The 30-year bond US30YT=RR grew 1-3/32 for a y...  \n",
       "\n",
       "[3962 rows x 3 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train_MSRPC_data =  pd.read_table(train_MSRPC_file, usecols=[1, 2, 3] , names = [ 'Acceptability judgment label' , 'Sentence'])\n",
    "train_MSRPC_data =  pd.read_csv(train_MSRPC_file, delimiter='\\t',  header=0,  usecols=[0,3,4] , names = [ 'Quality Score' , 'Sentence 1', 'Sentence 2'] )\n",
    "train_MSRPC_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 169,
     "status": "ok",
     "timestamp": 1635568242716,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "V5WIoM4Edq2_",
    "outputId": "3e60a513-ffaa-4c64-aa2b-feb2e6f67456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#todo, why did I find 3962 while it says on the web it should find 5,801 sentences ?? \n"
     ]
    }
   ],
   "source": [
    "print('#todo, why did I find 3962 while it says on the web it should find 5,801 sentences ?? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 152,
     "status": "ok",
     "timestamp": 1635567870089,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "yj5_Hr0Db-mn",
    "outputId": "6e3f8ac6-3e5d-46f6-e710-9d2998f33460"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2677\n",
       "0    1285\n",
       "Name: Quality Score, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_MSRPC_data['Quality Score'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    },
    "executionInfo": {
     "elapsed": 404,
     "status": "ok",
     "timestamp": 1635567888670,
     "user": {
      "displayName": "Mouna Dhaouadi",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "01193576242358899083"
     },
     "user_tz": 240
    },
    "id": "cBRaYk7QcKPu",
    "outputId": "93c71ed0-3639-4898-831c-1c6ec8c7461f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:xlabel='Quality Score', ylabel='count'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from seaborn import countplot\n",
    "countplot(x='Quality Score', data = train_MSRPC_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dhAg2GFfaupW"
   },
   "source": [
    "### Step 2 : Reasonable Baseline - distance edition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "IaJ498sPauL1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Amrozi accused his brother, whom he called \"the witness\", of deliberately distorting his evidence.\n",
      "nan\n",
      "53\n",
      "1\n",
      "Yucaipa owned Dominick's before selling the chain to Safeway in 1998 for $2.5 billion.\n",
      "Referring to him as only \"the witness\", Amrozi accused his brother of deliberately distorting his evidence.\n",
      "49\n",
      "2\n",
      "They had published an advertisement on the Internet on June 10, offering the cargo for sale, he added.\n",
      "Yucaipa bought Dominick's in 1995 for $693 million and sold it to Safeway for $1.8 billion in 1998.\n",
      "57\n",
      "3\n",
      "Around 0335 GMT, Tab shares were up 19 cents, or 4.4%, at A$4.56, having earlier set a record high of A$4.57.\n",
      "On June 10, the ship's owners had published an advertisement on the Internet, offering the explosives for sale.\n",
      "60\n",
      "4\n",
      "The stock rose $2.11, or about 11 percent, to close Friday at $21.51 on the New York Stock Exchange.\n",
      "Tab shares jumped 20 cents, or 4.6%, to set a record closing high at A$4.57.\n",
      "59\n",
      "5\n",
      "Revenue in the first quarter of the year dropped 15 percent from the same period a year earlier.\n",
      "PG&E Corp. shares jumped $1.63 or 8 percent to $21.03 on the New York Stock Exchange on Friday.\n",
      "50\n",
      "6\n",
      "The Nasdaq had a weekly gain of 17.27, or 1.2 percent, closing at 1,520.15 on Friday.\n",
      "With the scandal hanging over Stewart's company, revenue the first quarter of the year dropped 15 percent from the same period a year earlier.\n",
      "63\n",
      "7\n",
      "The DVD-CCA then appealed to the state Supreme Court.\n",
      "The tech-laced Nasdaq Composite .IXIC rallied 30.46 points, or 2.04 percent, to 1,520.15.\n",
      "24\n",
      "8\n",
      "That compared with $35.18 million, or 24 cents per share, in the year-ago period.\n",
      "The DVD CCA appealed that decision to the U.S. Supreme Court.\n",
      "55\n",
      "9\n",
      "He said the foodservice pie business doesn't fit the company's long-term growth strategy.\n",
      "Earnings were affected by a non-recurring $8 million tax benefit in the year-ago period.\n",
      "1260\n",
      "10\n",
      "Gyorgy Heizler, head of the local disaster unit, said the coach was carrying 38 passengers.\n",
      "The foodservice pie business does not fit our long-term growth strategy.\n",
      "0\t222621\t222514\tShares of Genentech, a much larger company with several products on the market, rose more than 2 percent.\tShares of Xoma fell 16 percent in early trade, while shares of Genentech, a much larger company with several products on the market, were up 2 percent.\n",
      "0\t3131772\t3131625\tLegislation making it harder for consumers to erase their debts in bankruptcy court won overwhelming House approval in March.\tLegislation making it harder for consumers to erase their debts in bankruptcy court won speedy, House approval in March and was endorsed by the White House.\n",
      "0\t58747\t58516\tThe Nasdaq composite index increased 10.73, or 0.7 percent, to 1,514.77.\tThe Nasdaq Composite index, full of technology stocks, was lately up around 18 points.\n",
      "1\t1464126\t1464107\tBut he added group performance would improve in the second half of the year and beyond.\tDe Sole said in the results statement that group performance would improve in the second half of the year and beyond.\n",
      "1\t771416\t771467\tHe told The Sun newspaper that Mr. Hussein's daughters had British schools and hospitals in mind when they decided to ask for asylum.\tSaddam's daughters had British schools and hospitals in mind when they decided to ask for asylum -- especially the schools,\" he told The Sun.\n",
      "63\n",
      "11\n",
      "Rudder was most recently senior vice president for the Developer & Platform Evangelism Business.\n",
      "The head of the local disaster unit, Gyorgy Heizler, said the coach driver had failed to heed red stop lights.\n",
      "72\n",
      "12\n",
      "As well as the dolphin scheme, the chaos has allowed foreign companies to engage in damaging logging and fishing operations without proper monitoring or export controls.\n",
      "Senior Vice President Eric Rudder, formerly head of the Developer and Platform Evangelism unit, will lead the new entity.\n",
      "50\n",
      "13\n",
      "Magnarelli said Racicot hated the Iraqi regime and looked forward to using his long years of training in the war.\n",
      "Internal chaos has allowed foreign companies to set up damaging commercial logging and fishing operations without proper monitoring or export controls.\n",
      "48\n",
      "14\n",
      "Sheena Young of Child, the national infertility support network, hoped the guidelines would lead to a more \"fair and equitable\" service for infertility sufferers.\n",
      "His wife said he was \"100 percent behind George Bush\" and looked forward to using his years of training in the war.\n",
      "29\n",
      "15\n",
      "I think you'll see a lot of job growth in the next two years, he said, adding the growth could replace jobs lost.\n",
      "Sheena Young, a spokesman for Child, the national infertility support network, said the proposed guidelines should lead to a more \"fair and equitable\" service for infertility sufferers.\n",
      "43\n",
      "16\n",
      "The new Finder puts a user's folders, hard drive, network servers, iDisk and removable media in one location, providing one-click access.\n",
      "I think you'll see a lot of job growth in the next two years, said Mankiw.\n",
      "69\n",
      "17\n",
      "But tropical storm warnings and watches were posted today for Haiti, western portions of the Dominican Republic, the southeastern Bahamas and the Turk and Caicos islands.\n",
      "Panther's redesigned Finder navigation tool puts a user's favourite folders, hard drive, network servers, iDisk and removable media in one location.\n",
      "60\n",
      "18\n",
      "A federal magistrate in Fort Lauderdale ordered him held without bail.\n",
      "Tropical storm warnings were in place Thursday for Jamaica and Haiti and watches for the western Dominican Republic, the southeastern Bahamas and the Turks and Caicos islands.\n",
      "69\n",
      "19\n",
      "A BMI of 25 or above is considered overweight; 30 or above is considered obese.\n",
      "Zuccarini was ordered held without bail Wednesday by a federal judge in Fort Lauderdale, Fla.\n",
      "60\n",
      "20\n",
      "The dollar was at 116.92 yen against the yen , flat on the session, and at 1.2891 against the Swiss franc , also flat.\n",
      "A BMI between 18.5 and 24.9 is considered normal, over 25 is considered overweight and 30 or greater is defined as obese.\n",
      "38\n",
      "21\n",
      "Six months ago, the IMF and Argentina struck a bare-minimum $6.8-billion debt rollover deal that expires in August.\n",
      "The dollar was at 116.78 yen JPY= , virtually flat on the session, and at 1.2871 against the Swiss franc CHF= , down 0.1 percent.\n",
      "41\n",
      "22\n",
      "Inhibited children tend to be timid with new people, objects, and situations, while uninhibited children spontaneously approach them.\n",
      "But six months ago, the two sides managed to strike a $6.8-billion debt rollover deal, which expires in August.\n",
      "94\n",
      "23\n",
      "I wanted to bring the most beautiful people into the most beautiful building, he said Sunday inside the Grand Central concourse.\n",
      "Simply put, shy invividuals tend to be more timid with new people and situations.\n",
      "40\n",
      "24\n",
      "The broad Standard & Poor's 500 <.SPX> fell 10.75 points, or 1.02 percent, to 1,039.32.\n",
      "I wanted to bring the most beautiful people into the most beautiful building, Tunick said Sunday.\n",
      "70\n",
      "25\n",
      "Duque will return to Earth Oct. 27 with the station's current crew, U.S. astronaut Ed Lu and Russian cosmonaut Yuri Malenchenko.\n",
      "The S&P 500 index was up 1.26, or 0.1 percent, to 1,039.32 after sinking 10.75 yesterday.\n",
      "48\n",
      "26\n",
      "Singapore is already the United States' 12th-largest trading partner, with two-way trade totaling more than $34 billion.\n",
      "Currently living onboard the space station are American astronaut Ed Lu and Russian cosmonaut Yuri Malenchenko.\n",
      "82\n",
      "27\n",
      "The AFL-CIO is waiting until October to decide if it will endorse a candidate.\n",
      "Although a small city-state, Singapore is the 12th-largest trading partner of the United States, with trade volume of $33.4 billion last year.\n",
      "66\n",
      "28\n",
      "No dates have been set for the civil or the criminal trial.\n",
      "The AFL-CIO announced Wednesday that it will decide in October whether to endorse a candidate before the primaries.\n",
      "43\n",
      "29\n",
      "The largest gains were seen in prices, new orders, inventories and exports.\n",
      "No dates have been set for the criminal or civil cases, but Shanley has pleaded not guilty.\n",
      "34\n",
      "30\n",
      "Trading in Loral was halted yesterday; the shares closed on Monday at $3.01.\n",
      "Sub-indexes measuring prices, new orders, inventories and exports increased.\n",
      "66\n",
      "31\n",
      "Earnings per share from recurring operations will be 13 cents to 14 cents.\n",
      "The New York Stock Exchange suspended trading yesterday in Loral, which closed at $3.01 Friday.\n",
      "52\n",
      "32\n",
      "He plans to have dinner with troops at Kosovo's U.S. military headquarters, Camp Bondsteel.\n",
      "That beat the company's April earnings forecast of 8 to 9 cents a share.\n",
      "63\n",
      "33\n",
      "Retailers J.C. Penney Co. Inc. (JCP) and Walgreen Co. (WAG) kick things off on Monday.\n",
      "After that, he plans to have dinner at Camp Bondsteel with U.S. troops stationed there.\n",
      "6\n",
      "34\n",
      "Prosecutors filed a motion informing Lee they intend to seek the death penalty.\n",
      "Retailers J.C. Penney Co. Inc. JCP.N and Walgreen Co. WAG.N kick things off on Monday.\n",
      "44\n",
      "35\n",
      "Last year the court upheld Cleveland's school voucher program, ruling 5-4 that vouchers are constitutional if they provide parents a choice of religious and secular schools.\n",
      "He added that prosecutors will seek the death penalty.\n",
      "64\n",
      "36\n",
      "He beat testicular cancer that had spread to his lungs and brain.\n",
      "Last year, the court ruled 5-4 in an Ohio case that government vouchers are constitutional if they provide parents with choices among a range of religious and secular schools.\n",
      "33\n",
      "37\n",
      "Sorkin, who faces charges of conspiracy to obstruct justice and lying to a grand jury, was to have been tried separately.\n",
      "Armstrong, 31, battled testicular cancer that spread to his brain.\n",
      "79\n",
      "38\n",
      "Graves reported from Albuquerque, Villafranca from Austin and Ratcliffe from Laredo.\n",
      "Sorkin was to have been tried separately on charges of conspiracy and lying to a grand jury.\n",
      "59\n",
      "39\n",
      "The US chip market is expected to decline 2.1 percent this year, then grow 15.7 percent in 2004.\n",
      "Pete Slover reported from Laredo and Gromer Jeffers from Albuquerque.\n",
      "64\n",
      "40\n",
      "The group will be headed by State Department official John S. Wolf, who has served in Australia, Vietnam, Greece and Pakistan.\n",
      "The Americas market will decline 2.1 percent to $30.6 billion in 2003, and then grow 15.7 percent to $35.4 billion in 2004.\n",
      "39\n",
      "41\n",
      "The commission must work out the plan's details, but the average residential customer paying $840 a year would get a savings of about $30 annually.\n",
      "The group will be headed by John S. Wolf, an assistant secretary of state who has served in Australia, Vietnam, Greece and Pakistan.\n",
      "80\n",
      "42\n",
      "The company has said it plans to restate its earnings for 2000 through 2002.\n",
      "An average residential customer paying $840 a year for electricity could see a savings of $30 annually.\n",
      "54\n",
      "43\n",
      "Results from No. 2 U.S. soft drink maker PepsiCo Inc. PEP.N were likely to be in the spotlight.\n",
      "The company had announced in January that it would have to restate earnings for 2002, 2001 and perhaps 2000.\n",
      "24\n",
      "44\n",
      "The result is an overall package that will provide significant economic growth for our employees over the next four years.\n",
      "Results from No. 2 U.S. soft drink maker PepsiCo Inc. (nyse: PEP - news - people) were likely to be in the spotlight.\n",
      "14\n",
      "45\n",
      "Wal-Mart said it would check all of its million-plus domestic workers to ensure they were legally employed.\n",
      "The result is an overall package that will provide a significant economic growth for our employees over the next few years, he said.\n",
      "57\n",
      "46\n",
      "The songs are on offer for 99 cents each, or $9.99 for an album.\n",
      "It has also said it would review all of its domestic employees more than 1 million to ensure they have legal status.\n",
      "37\n",
      "47\n",
      "However, the talk was downplayed by PBL which said it would focus only on smaller purchases that were immediately earnings and cash flow accretive.\n",
      "The company will offer songs for 99 cents and albums for $9.95.\n",
      "23\n",
      "48\n",
      "Comcast Class A shares were up 8 cents at $30.50 in morning trading on the Nasdaq Stock Market.\n",
      "The talk, however,has been downplayed by PBL which said it would focus only on smaller purchases that were immediately earnings and cash flow-accretive.\n",
      "58\n",
      "49\n",
      "While dioxin levels in the environment were up last year, they have dropped by 75 percent since the 1970s, said Caswell.\n",
      "The stock rose 48 cents to $30 yesterday in Nasdaq Stock Market trading.\n",
      "59\n",
      "50\n",
      "This integrates with Rational PurifyPlus and allows developers to work in supported versions of Java, Visual C# and Visual Basic .NET.\n",
      "The Institute said dioxin levels in the environment have fallen by as much as 76 percent since the 1970s.\n",
      "69\n",
      "51\n",
      "The Washington Post said Airlite would shut down its first shift and parts of the second shift Monday to accommodate the president’s appearance.\n",
      "IBM said the Rational products were also integrated with Rational PurifyPlus, which allows developers to work in Java, Visual C# and VisualBasic .Net.\n",
      "41\n",
      "52\n",
      "A former teammate, Carlton Dotson, has been charged with the murder.\n",
      "The plant plans to shut down its first shift and parts of the second shift Monday to accommodate the president's appearance, Crosby said.\n",
      "49\n",
      "53\n",
      "Several of the questions asked by the audience in the fast-paced forum were new to the candidates.\n",
      "His body was found July 25, and former teammate Carlton Dotson has been charged in his shooting death.\n",
      "50\n",
      "54\n",
      "Meanwhile, the global death toll approached 770 with more than 8,300 people sickened since the severe acute respiratory syndrome virus first appeared in southern China in November.\n",
      "Several of the audience questions were new to the candidates as well.\n",
      "68\n",
      "55\n",
      "The battles marked day four of a U.S. sweep to hunt down supporters of Saddam Hussein's fallen regime.\n",
      "The global death toll from SARS was at least 767, with more than 8,300 people sickened since the virus first appeared in southern China in November.\n",
      "79\n",
      "56\n",
      "The women then had follow-up examinations after five, 12 and 24 years.\n",
      "Twenty-seven Iraqis were killed, pushing the number of opposition deaths to about 100 in a U.S. operation to hunt down supporters of Saddam Hussein's fallen regime.\n",
      "61\n",
      "57\n",
      "The Embraer jets are scheduled to be delivered by September 2006.\n",
      "The women had follow-up examinations in 1974-75, 1980-81 and 1992-93, but were not asked about stress again.\n",
      "43\n",
      "58\n",
      "Contrary to what PeopleSoft management would have you believe, Oracle intends to fully support PeopleSoft customers and products for many years to come.\"\n",
      "The Bombardier and Embraer aircraft will be delivered to U.S. Airways by September 2006.\n",
      "57\n",
      "59\n",
      "Application Intelligence will be included as part of the company's SmartDefense application, which is included with Firewall-1.\n",
      "Ellison said that contrary to the contentions of PeopleSoft management, Oracle intends to \"fully support PeopleSoft customers and products\" for many years to come.\n",
      "70\n",
      "60\n",
      "American Masters: Arthur Miller, Elia Kazan and the Blacklist: None Without Sin (Wed.\n",
      "The new application intelligence features will be available June 3 and are included with the SmartDefense product, which comes with FireWall-1.\n",
      "69\n",
      "61\n",
      "The downtime, to take place in May and June, is expected to cut production by 60 million to 70 million board feet.\n",
      "Note the subheading of this terrible parable in the \"American Masters\" series, \"Arthur Miller, Elia Kazan and the Blacklist: None Without Sin.\"\n",
      "75\n",
      "62\n",
      "On July 3, Troy is expected to be sentenced to life in prison without parole.\n",
      "The downtime is expected to take 60 million to 70 million board feet out of the companys system.\n",
      "61\n",
      "63\n",
      "The University of Michigan released a new undergraduate admission process Thursday, dropping a point system the U.S. Supreme Court found unconstitutional in June.\n",
      "Troy faces life in prison without parole at his July 30 sentencing.\n",
      "99\n",
      "64\n",
      "The processors were announced in San Jose at the Intel Developer Forum.\n",
      "The University of Michigan released today a new admissions policy after the U.S. Supreme Court struck down in June the way it previously admitted undergraduates.\n",
      "49\n",
      "65\n",
      "The Justice Department filed suit Thursday against the state of Mississippi for failing to end what federal officials call \"disturbing\" abuse of juveniles and \"unconscionable\" conditions at two state-run facilities.\n",
      "The new processor was unveiled at the Intel Developer Forum 2003 in San Jose, Calif.\n",
      "98\n",
      "66\n",
      "It said the damage to the wing provided a pathway for hot gasses to penetrate the ship's thermal armor during Columbia's ill-fated reentry.\n",
      "The Justice Department filed a civil rights lawsuit Thursday against the state of Mississippi, alleging abuse of juvenile offenders at two state-run facilities.\n",
      "37\n",
      "67\n",
      "Also demonstrating box-office strength _ and getting seven Tony nominations _ was a potent revival of Eugene O'Neill's family drama, \"Long Day's Journey Into Night.\"\n",
      "The document says the damage to the wing provided a pathway for hot gases to penetrate Columbia's thermal armour during its fatal re-entry.\n",
      "8\n",
      "68\n",
      "The top rate will go to 4.45 percent for all residents with taxable incomes above $500,000.\n",
      "Also demonstrating box-office strength -- and getting seven Tony nominations -- was a potent revival of Eugene ONeills family drama, Long Days Journey Into Night.\"\n",
      "73\n",
      "69\n",
      "But Secretary of State Colin Powell brushed off this possibility Wednesday.\n",
      "For residents with incomes above $500,000, the income-tax rate will increase to 4.45 percent.\n",
      "40\n",
      "70\n",
      "Thomas and Tauzin say, as do many doctors, that the Bush administration has the power to correct some of those flaws.\n",
      "Secretary of State Colin Powell last week ruled out a non-aggression treaty.\n",
      "40\n",
      "71\n",
      "Based on experience elsewhere, it could take up to two years before regular elections are held, he added.\n",
      "Like many doctors, Mr. Thomas and Mr. Tauzin say the Bush administration has the power to correct some of those flaws.\n",
      "68\n",
      "72\n",
      "The results appear in the January issue of Cancer, an American Cancer Society journal, being published online today.\n",
      "U.S. military officials have said it could take up to two years before regular elections are held, based on experiences elsewhere in the world.\n",
      "21\n",
      "73\n",
      "The first biotechnology treatment for asthma, the constriction of the airways that affects millions around the world, received approval from the US Food and Drug Administration yesterday.\n",
      "The results appear in the January issue of Cancer, an American Cancer Society (news - web sites) journal, being published online Monday.\n",
      "21\n",
      "74\n",
      "The delegates said raising and distributing funds has been complicated by the U.S. crackdown on jihadi charitable foundations, bank accounts of terror-related organizations and money transfers.\n",
      "The first biotechnology treatment for asthma, the constriction of the airways that affects millions of Americans, received approval from the U.S. Food and Drug Administration on Friday.\n",
      "25\n",
      "75\n",
      "FBI agents arrested a former partner of Big Four accounting firm Ernst & Young ERNY.UL on criminal charges of obstructing federal investigations, U.S. officials said on Thursday.\n",
      "Bin Laden’s men pointed out that raising and distributing funds has been complicated by the U.S. crackdown on jihadi charitable foundations, bank accounts of terror-related organizations and money transfers.\n",
      "111\n",
      "76\n",
      "Kelly will begin meetings with Russian Deputy Foreign Minister Alexander Losyukov in Washington on Monday.\n",
      "A former partner of accountancy firm Ernst & Young was yesterday arrested by FBI agents in the US on charges of obstructing federal investigations.\n",
      "88\n",
      "77\n",
      "The latest shooting linked to the spree was a November 11 shooting at Hamilton Central Elementary School in Obetz, about 3km from the freeway.\n",
      "Russian Deputy Foreign Minister Alexander Losyukov said in Moscow Tuesday a firm date would be fixed by this months end.\n",
      "46\n",
      "78\n",
      "Sanitation is poor... there could be typhoid and cholera, he said.\n",
      "Another shooting linked to the spree occurred Nov. 11 at Hamilton Central Elementary in Obetz, about two miles from the freeway.\n",
      "53\n",
      "79\n",
      "The Dow Jones Industrial Average ended down 128 points, or 1.4%, at 9073, while the Nasdaq fell 34 points, or 2.1%, to 1610.\n",
      "Sanitation is poor, drinking water is generally left behind . . . there could be typhoid and cholera.\n",
      "93\n",
      "80\n",
      "PDC will also almost certainly fan the flames of speculation about Longhorn's release.\n",
      "In early trading, the Dow Jones industrial average was up 3.90, or 0.04 percent, at 9,113.75, having gained 36.90 on Tuesday.\n",
      "50\n",
      "81\n",
      "Sales - a figure watched closely as a barometer of its health - rose 5 percent instead of falling as many industry experts had predicted.\n",
      "PDC will also almost certainly reignite speculation about release dates of Microsoft's new products.\n",
      "81\n",
      "82\n",
      "NEC is pitching its wireless gear and management software to a variety of industries, including health care and hospitality.\n",
      "It also disclosed that sales -- a figure closely watched by analysts as a barometer of its health -- were significantly higher than industry experts expected.\n",
      "29\n",
      "83\n",
      "Elena Slough, considered to be the nation's oldest person and the third oldest person in the world, died early Sunday morning.\n",
      "NEC's pitching its wireless gear and management software to a variety of industries, including healthcare and hospitality, a company spokesman said.\n",
      "44\n",
      "84\n",
      "We are declaring war on sexual harassment and sexual assault.\tWe have declared war on sexual assault and sexual harassment,\" Rosa said.\n",
      "ELENA Slough, considered to be the oldest person in the US and the third oldest person in the world, has died.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'float' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Roaming\\MobaXterm\\slash\\var\\log\\xwin/ipykernel_24676/2852994069.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'\\n'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0msent2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_MSRPC_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Sentence 2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0meditdistance\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msent1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msent2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mevals\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mev\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32meditdistance/bycython.pyx\u001b[0m in \u001b[0;36meditdistance.bycython.eval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32meditdistance/bycython.pyx\u001b[0m in \u001b[0;36meditdistance.bycython.eval\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'float' has no len()"
     ]
    }
   ],
   "source": [
    "import editdistance\n",
    "evals = []\n",
    "for i in range(len(train_MSRPC_data['Sentence 1'])):\n",
    "    print(i)\n",
    "    sent1 = train_MSRPC_data['Sentence 1'][i]\n",
    "    print(sent1,sent2,sep='\\n')\n",
    "    sent2 = train_MSRPC_data['Sentence 2'][i]\n",
    "    ev = editdistance.eval(sent1,sent2)\n",
    "    print(ev)\n",
    "    evals.append(ev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "4DO0e7sbaoNG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3962"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_MSRPC_data['Sentence 1'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPsL1OkXaoF-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ED8luFna-3k"
   },
   "source": [
    "### Step 3 : Improving the baseline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lU1ZjqJYan_z"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HPYn7J-aanwT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6SgGe18ybDYs"
   },
   "source": [
    "### Step 4: Choosing the best system "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "auuoNe6QbOw3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r3CqKXgRbOqu"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vh3nj7nHbPmX"
   },
   "source": [
    "### Step 5 : Neat Analysis of the chosen system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c97xG_Fjbjbw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe8w-VDFbjvV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XQ47PpHabVmV"
   },
   "source": [
    "### Step 6 : Search for weaknesseness in the system and try to correct them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zCerVFXIbVNI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjcZRBVRbmON"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DdFMy-L7bvad"
   },
   "source": [
    "### Step 7 : Testing the system on new data (later)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mQZsAjHeb0un"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBeuPVLubRas"
   },
   "source": [
    "# The end."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "HLbZswvJ3jxz",
    "A3rZklEFUw4C"
   ],
   "name": "Projet1-Classification - task2.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "interpreter": {
   "hash": "e5b6df043f971341f90cd2889fa6099d6abf6d912c177742f46aa7e6424054fa"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
